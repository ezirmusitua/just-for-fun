{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator For Regression  \n",
    "`make_regression` - produces regression targets as an optionally-sparse random linear combination of random features, with noise.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function sklearn.datasets.samples_generator.make_regression(n_samples=100, n_features=100, n_informative=10, n_targets=1, bias=0.0, effective_rank=None, tail_strength=0.5, noise=0.0, shuffle=True, coef=False, random_state=None)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.make_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`make_sparse_uncorrelated` - produces a target as a linear combination of four features with fixed coefficients  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function sklearn.datasets.samples_generator.make_sparse_uncorrelated(n_samples=100, n_features=10, random_state=None)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.make_sparse_uncorrelated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`make_friedman1` - related by polynomial and sine transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function sklearn.datasets.samples_generator.make_friedman1(n_samples=100, n_features=10, noise=0.0, random_state=None)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.make_friedman1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`make_friedman2` -  includes feature multiplication and reciprocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function sklearn.datasets.samples_generator.make_friedman2(n_samples=100, noise=0.0, random_state=None)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.make_friedman2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`make_friedman3` - similar with an arctan transformation on the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function sklearn.datasets.samples_generator.make_friedman3(n_samples=100, noise=0.0, random_state=None)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.make_friedman3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator For Manifold Learning  \n",
    "`make_s_curve` - Generate an S curve dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function sklearn.datasets.samples_generator.make_s_curve(n_samples=100, noise=0.0, random_state=None)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.make_s_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`make_swiss_roll` - Generate a swiss roll dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function sklearn.datasets.samples_generator.make_swiss_roll(n_samples=100, noise=0.0, random_state=None)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.make_swiss_roll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator For Decomposition  \n",
    "`make_low_rank_matrix` - Generate a mostly low rank matrix with bell-shaped singular values  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function sklearn.datasets.samples_generator.make_low_rank_matrix(n_samples=100, n_features=100, effective_rank=10, tail_strength=0.5, random_state=None)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.make_low_rank_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`make_sparse_coded_signal` - Generate a signal sas a sparse combination of dictionary elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function sklearn.datasets.samples_generator.make_sparse_coded_signal(n_samples, n_components, n_features, n_nonzero_coefs, random_state=None)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.make_sparse_coded_signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`make_spd_matrix` - Generate a random symmetric, positive-definite matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function sklearn.datasets.samples_generator.make_spd_matrix(n_dim, random_state=None)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.make_spd_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`make_sparse_spd_matrix` - Generate a sparse symmetric definite positive matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function sklearn.datasets.samples_generator.make_sparse_spd_matrix(dim=1, alpha=0.95, norm_diag=False, smallest_coef=0.1, largest_coef=0.9, random_state=None)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.make_sparse_spd_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets in svmlight / libsvm format \n",
    "svmlight/libsvm format: \\<label\\>  \\<feature-id\\>:\\<feature-value\\> \\<feature-id\\>: \\<feature-value\\> per line  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function sklearn.datasets.svmlight_format.load_svmlight_file(f, n_features=None, dtype=<class 'numpy.float64'>, multilabel=False, zero_based='auto', query_id=False, offset=0, length=-1)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.load_svmlight_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading From Extrenal Datasets  \n",
    "1. pandas.io\n",
    "2. scipy.io  \n",
    "3. numpy/routine.io  \n",
    "4. skimage.io / Imageio  \n",
    "5. scipy.misc.imread  \n",
    "6. scipy.io.wavfile.read  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading datasets from the mldata.org repository  \n",
    "`fetch_mldata`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function sklearn.datasets.mldata.fetch_mldata(dataname, target_name='label', data_name='data', transpose_data=True, data_home=None)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.fetch_mldata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Transformations  \n",
    "clean, reduce, expand, generate feature representations  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline And FeatureUnion: Cobining Estimators  \n",
    "Pipeline can use to chain estimators into one, useful when there is ofen a fixed sequence of steps in processing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to use  \n",
    "`Pipeline(estimators)`, estimators is a list of (key, value) tuple, key is the name of estimator, value is the estimator object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('reduce_im', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)), ('clf', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA  \n",
    "estimators = [('reduce_im', PCA()), ('clf', SVC())]  \n",
    "pipe = Pipeline(estimators)\n",
    "pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`make_pipeline(estimator1, estimator, ...)` shorthand for creating pipeline, name was autofilled  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('pca-1', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)), ('pca-2', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)), ('svc', SVC(C=1.0, cache_size...,\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline  \n",
    "pipe = make_pipeline(PCA(), PCA(), SVC())\n",
    "pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**steps/named_steps** attribute store the estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pca-1',\n",
       "  PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
       "    svd_solver='auto', tol=0.0, whiten=False)),\n",
       " ('pca-2',\n",
       "  PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
       "    svd_solver='auto', tol=0.0, whiten=False)),\n",
       " ('svc', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False))]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pca-1': PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
       "   svd_solver='auto', tol=0.0, whiten=False),\n",
       " 'pca-2': PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
       "   svd_solver='auto', tol=0.0, whiten=False),\n",
       " 'svc': SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "   decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "   max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "   tol=0.001, verbose=False)}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.named_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use **estimator__parameter** to access estimator's parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('pca-1', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)), ('pca-2', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)), ('svc', SVC(C=0, cache_size=2...,\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.set_params(svc__C=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following shows how to use GridSearchCV  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'pca-1__n_components': [2, 5, 10], 'svc__C': [0.1, 10, 100]}\n",
    "grid_search = GridSearchCV(pipe, param_grid=param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perfermance  \n",
    "pipeline will cache each transform after calling fit, so if parameters and input data are identical the tranformation wont running  \n",
    "`Pipeline(..., memory=dirname_or_jobmemoryobject)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempfile import mkdtemp\n",
    "from shutil import rmtree\n",
    "estimators = [('reduce_dim', PCA()), ('clf', SVC())]\n",
    "cachedir = mkdtemp()\n",
    "pipe = Pipeline(estimators, memory=cachedir)\n",
    "rmtree(cachedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FeatureUnion: Composite Feature Spaces  \n",
    "FeatureUnion combine several transformer objects into a new transformer that combines their output  \n",
    "while fitting, each estimator fit data indenpendently, the output are concatenated end-to-end into larger vectors  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('linear_pca', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)), ('kernel_pca', KernelPCA(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto',\n",
       "     fit_inverse_transform=False, gamma=None, kernel='linear',\n",
       "     kernel_params=None, max_iter=None, n_components=None, n_jobs=1,\n",
       "     random_state=None, remove_zero_eig=False, tol=0))],\n",
       "       transformer_weights=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.decomposition import KernelPCA\n",
    "estimators = [('linear_pca', PCA()), ('kernel_pca', KernelPCA())]\n",
    "combined = FeatureUnion(estimators)\n",
    "combined "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction  \n",
    "The sklearn.feature_extraction module can be used to extract features in a format supported by machine learning algorithms from datasets consisting of formats such as text and image.\n",
    "**Difference with Feature Selection**: the former consists in transforming arbitrary data, such as text or images, into numerical features usable for machine learning. The latter is a machine learning technique applied on these features  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Features From Dicts  \n",
    "`DictVectorizer` - convert feature arrays represented as liss of standard python dict object to numpy/scipy representation, it implement the `one-hot` coding for categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0. 33.]\n",
      " [ 0.  1.  0. 12.]\n",
      " [ 0.  0.  1. 18.]]\n",
      "['city=Dubai', 'city=London', 'city=San Francisco', 'temperature']\n"
     ]
    }
   ],
   "source": [
    ">>> measurements = [\n",
    "...     {'city': 'Dubai', 'temperature': 33.},\n",
    "...     {'city': 'London', 'temperature': 12.},\n",
    "...     {'city': 'San Francisco', 'temperature': 18.},\n",
    "... ]\n",
    "\n",
    ">>> from sklearn.feature_extraction import DictVectorizer\n",
    ">>> vec = DictVectorizer()\n",
    "\n",
    ">>> print(vec.fit_transform(measurements).toarray())\n",
    "\n",
    ">>> print(vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also is a useful representation transformation for training sequence classifiers in NLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1.0\n",
      "  (0, 1)\t1.0\n",
      "  (0, 2)\t1.0\n",
      "  (0, 3)\t1.0\n",
      "  (0, 4)\t1.0\n",
      "  (0, 5)\t1.0\n",
      "[[1. 1. 1. 1. 1. 1.]]\n",
      "['pos+1=PP', 'pos-1=NN', 'pos-2=DT', 'word+1=on', 'word-1=cat', 'word-2=the']\n"
     ]
    }
   ],
   "source": [
    ">>> pos_window = [\n",
    "...     {\n",
    "...         'word-2': 'the',\n",
    "...         'pos-2': 'DT',\n",
    "...         'word-1': 'cat',\n",
    "...         'pos-1': 'NN',\n",
    "...         'word+1': 'on',\n",
    "...         'pos+1': 'PP',\n",
    "...     },\n",
    "...     # in a real application one would extract many such dictionaries\n",
    "... ]\n",
    ">>> vec = DictVectorizer()\n",
    ">>> pos_vectorized = vec.fit_transform(pos_window)\n",
    ">>> print(pos_vectorized)\n",
    ">>> print(pos_vectorized.toarray())\n",
    ">>> print(vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Hashing  \n",
    "An implementation of Feature hashing, apply a hash function to features to determine their column index in sample matrices directly  \n",
    "About collisions: use a signed hash function   \n",
    "Accept mappings or (feature, value) pair or strings  \n",
    "Output scipy.sparse  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 729803)\t-1.0\n",
      "  (0, 740061)\t1.0\n",
      "  (0, 892359)\t-1.0\n",
      "  (0, 950346)\t-1.0\n",
      "  (0, 1002789)\t-1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import FeatureHasher\n",
    "def token_features(token, part_of_speech):\n",
    "    if token.isdigit():\n",
    "        yield \"numeric\"\n",
    "    else:\n",
    "        yield \"token={}\".format(token.lower())\n",
    "        yield \"token,pos={},{}\".format(token, part_of_speech)\n",
    "    if token[0].isupper():\n",
    "        yield \"uppercase_initial\"\n",
    "    if token.isupper():\n",
    "        yield \"all_uppercase\"\n",
    "    yield \"pos={}\".format(part_of_speech)\n",
    "raw_X = (token_features(tok, pos) for tok, pos in [('A', 5)])\n",
    "hasher = FeatureHasher(input_type='string')\n",
    "X = hasher.transform(raw_X)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Feature Extraction  \n",
    "Extract numerical features from text content - **Vercorization**   \n",
    "Bag of words/Bag of n-grams Representation  \n",
    "1. tokenizing  \n",
    "2. counting  \n",
    "3. normalizing  \n",
    "\n",
    "#### Sparsity  \n",
    "Words in documents is a very small subset, use sparse to store it in order to save memory and fasten \n",
    "\n",
    "#### CountVectorizer  \n",
    "Implement both tokenization and occurrence counting  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 9)\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    ">>> corpus = [\n",
    "...     'This is the first document.',\n",
    "...     'This is the second second document.',\n",
    "...     'And the third one.',\n",
    "...     'Is this the first document?',\n",
    "... ]\n",
    ">>> X = vectorizer.fit_transform(corpus)\n",
    "print(X.shape)  \n",
    ">>> analyze = vectorizer.build_analyzer()\n",
    ">>> analyze(\"This is a text document to analyze.\") == (\n",
    "...     ['this', 'is', 'text', 'document', 'to', 'analyze'])\n",
    ">>> vectorizer.get_feature_names() == (\n",
    "...     ['and', 'document', 'first', 'is', 'one',\n",
    "...      'second', 'the', 'third', 'this'])\n",
    ">>> print(vectorizer.vocabulary_.get('document'))\n",
    ">>> vectorizer.transform(['Something completely new.']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract 2-grams of words in order to preserve some of the local ordering infomation  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0],\n",
       "       [0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> bigram_vectorizer = CountVectorizer(ngram_range=(1, 2),\n",
    "...                                     token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    ">>> analyze = bigram_vectorizer.build_analyzer()\n",
    ">>> analyze('Bi-grams are cool!') == (\n",
    "...     ['bi', 'grams', 'are', 'cool', 'bi grams', 'grams are', 'are cool'])\n",
    ">>> X_2 = bigram_vectorizer.fit_transform(corpus).toarray()\n",
    ">>> X_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF Term Weighting  \n",
    "Why? some words will be very present (e.g. “the”, “a”, “is” in English) hence carrying very little meaningful information about the actual contents of the document  \n",
    "What? \n",
    "> tf-idf(t, d) = tf(t, d) x idf(t)  \n",
    "> tf means term-frequency, idf means term-frequency times inverse document-frequency  \n",
    "> idf(t) = log((1+n<d\\>) / (1+df(d, t)) + 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.38629436 2.38629436 1.28768207 1.69314718 1.69314718 1.28768207\n",
      " 1.69314718 2.38629436 2.38629436 2.38629436 2.38629436 2.38629436\n",
      " 1.         1.69314718 2.38629436 2.38629436 2.38629436 2.38629436\n",
      " 1.28768207 1.69314718 2.38629436]\n",
      "[[0.         0.         0.28574186 0.37571621 0.37571621 0.28574186\n",
      "  0.37571621 0.         0.         0.         0.         0.\n",
      "  0.22190405 0.37571621 0.         0.         0.         0.\n",
      "  0.28574186 0.37571621 0.        ]\n",
      " [0.         0.         0.1793146  0.         0.         0.1793146\n",
      "  0.23577716 0.         0.         0.66460105 0.33230052 0.33230052\n",
      "  0.13925379 0.         0.33230052 0.         0.         0.\n",
      "  0.1793146  0.23577716 0.        ]\n",
      " [0.40240191 0.40240191 0.         0.         0.         0.\n",
      "  0.         0.         0.40240191 0.         0.         0.\n",
      "  0.16863046 0.         0.         0.40240191 0.40240191 0.40240191\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.25271307 0.33228732 0.33228732 0.25271307\n",
      "  0.         0.4683204  0.         0.         0.         0.\n",
      "  0.19625424 0.33228732 0.         0.         0.         0.\n",
      "  0.25271307 0.         0.4683204 ]]\n"
     ]
    }
   ],
   "source": [
    ">>> from sklearn.feature_extraction.text import TfidfTransformer\n",
    ">>> transformer = TfidfTransformer(smooth_idf=False)\n",
    ">>> tfidf = transformer.fit_transform(X_2)\n",
    ">>> print(transformer.idf_)\n",
    ">>> print(tfidf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoding Text Files  \n",
    "`CountVectorizer`take the encoding params(default is utf-8)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n"
     ]
    }
   ],
   "source": [
    ">>> import chardet    \n",
    ">>> text1 = b\"Sei mir gegr\\xc3\\xbc\\xc3\\x9ft mein Sauerkraut\"\n",
    ">>> text2 = b\"holdselig sind deine Ger\\xfcche\"\n",
    ">>> text3 = b\"\\xff\\xfeA\\x00u\\x00f\\x00 \\x00F\\x00l\\x00\\xfc\\x00g\\x00e\\x00l\\x00n\\x00 \\x00d\\x00e\\x00s\\x00 \\x00G\\x00e\\x00s\\x00a\\x00n\\x00g\\x00e\\x00s\\x00,\\x00 \\x00H\\x00e\\x00r\\x00z\\x00l\\x00i\\x00e\\x00b\\x00c\\x00h\\x00e\\x00n\\x00,\\x00 \\x00t\\x00r\\x00a\\x00g\\x00 \\x00i\\x00c\\x00h\\x00 \\x00d\\x00i\\x00c\\x00h\\x00 \\x00f\\x00o\\x00r\\x00t\\x00\"\n",
    ">>> decoded = [x.decode(chardet.detect(x)['encoding'])\n",
    "...            for x in (text1, text2, text3)]        \n",
    ">>> v = CountVectorizer().fit(decoded).vocabulary_    \n",
    ">>> for term in v: print(v)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More  \n",
    "1. [Applications And Examples](http://scikit-learn.org/stable/modules/feature_extraction.html#applications-and-examples)  \n",
    "2. [Limitations of the Bag of Words Representation](http://scikit-learn.org/stable/modules/feature_extraction.html#limitations-of-the-bag-of-words-representation)  \n",
    "3. [Vectorizing a large text corpus with hashing trick](http://scikit-learn.org/stable/modules/feature_extraction.html#vectorizing-a-large-text-corpus-with-the-hashing-trick)  \n",
    "4. [Performing out-of-core scaling with HashingVectorizer](http://scikit-learn.org/stable/modules/feature_extraction.html#performing-out-of-core-scaling-with-hashingvectorizer)  \n",
    "5. [Customizing the vectorizer classes](http://scikit-learn.org/stable/modules/feature_extraction.html#customizing-the-vectorizer-classes)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Feature Extract  \n",
    "#### Patch Extraction  \n",
    "`extract_patches_2d` - extract patches from image stored as a 2d array/3d(color) array  \n",
    "`reconstruct_from_patches_2d` - reconstruct image  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  3  6  9]\n",
      " [12 15 18 21]\n",
      " [24 27 30 33]\n",
      " [36 39 42 45]]\n",
      "(2, 2, 2, 3)\n",
      "[[[ 0  3]\n",
      "  [12 15]]\n",
      "\n",
      " [[15 18]\n",
      "  [27 30]]]\n",
      "(9, 2, 2, 3)\n",
      "[[15 18]\n",
      " [27 30]]\n"
     ]
    }
   ],
   "source": [
    ">>> import numpy as np\n",
    ">>> from sklearn.feature_extraction import image\n",
    ">>> one_image = np.arange(4 * 4 * 3).reshape((4, 4, 3))\n",
    ">>> print(one_image[:, :, 0])  # R channel of a fake RGB picture\n",
    ">>> patches = image.extract_patches_2d(one_image, (2, 2), max_patches=2, random_state=0)\n",
    ">>> print(patches.shape)\n",
    ">>> print(patches[:, :, :, 0])\n",
    ">>> patches = image.extract_patches_2d(one_image, (2, 2))\n",
    ">>> print(patches.shape)\n",
    ">>> print(patches[4, :, :, 0])\n",
    ">>> reconstructed = image.reconstruct_from_patches_2d(patches, (4, 4, 3))\n",
    ">>> np.testing.assert_array_equal(one_image, reconstructed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PatchExtractor` - like patch extractor, but support mult-image, is an estimator, can be use in pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 2, 2, 3)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> five_images = np.arange(5 * 4 * 4 * 3).reshape(5, 4, 4, 3)\n",
    ">>> patches = image.PatchExtractor((2, 2)).transform(five_images)\n",
    ">>> patches.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connectivity Graph of an Image  \n",
    "[Detail](http://scikit-learn.org/stable/modules/feature_extraction.html#connectivity-graph-of-an-image)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Data  \n",
    "standardize data\n",
    "#### Standardization or mean removal and variance scaling  \n",
    "In practice we often ignore the shape of the distribution and just **transform the data to center it by removing the mean value of each feature**, then **scale it by dividing non-constant features by their standard deviation**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scale` - a quick and easy way to perform standardization, scaled data has zero mean and unit variance \n",
    "`StandardScaler` - kind of estimator that implement scale  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         -1.22474487  1.33630621]\n",
      " [ 1.22474487  0.         -0.26726124]\n",
      " [-1.22474487  1.22474487 -1.06904497]]\n",
      "[1.         0.         0.33333333]\n",
      "[[-2.44948974  1.22474487 -0.26726124]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import scale as sk_scale, normalize as sk_normalize, StandardScaler,MinMaxScaler,MaxAbsScaler,QuantileTransformer,Normalizer,Binarizer, OneHotEncoder\n",
    "import numpy as np\n",
    "X_train = np.array([[ 1., -1.,  2.], [ 2.,  0.,  0.], [ 0.,  1., -1.]])\n",
    "X_scaled = sk_scale(X_train)\n",
    "print(X_scaled)  \n",
    "scaler = StandardScaler().fit(X_train)\n",
    "print(scaler.mean_)\n",
    "X_test = [[-1., 1., 0.]]\n",
    "print(scaler.transform(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MinMaxScaler/MaxAbsScaler` - scale features to lie between a given minimum and maximum value  \n",
    "For `MinMaxScaler`  \n",
    "> X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
    "> X_scaled = X_std * (max - min) + min  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 0.        , 1.        ],\n",
       "       [1.        , 0.5       , 0.33333333],\n",
       "       [0.        , 1.        , 0.        ]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> X_train = np.array([[ 1., -1.,  2.],[ 2.,  0.,  0.],[ 0.,  1., -1.]])\n",
    ">>> min_max_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    ">>> X_train_minmax = min_max_scaler.fit_transform(X_train)\n",
    ">>> X_train_minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5 -1.   1. ]\n",
      " [ 1.   0.   0. ]\n",
      " [ 0.   1.  -0.5]]\n",
      "[[-1.5 -1.   2. ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2., 1., 2.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> X_train = np.array([[ 1., -1.,  2.],[ 2.,  0.,  0.],[ 0.,  1., -1.]])\n",
    ">>> max_abs_scaler = MaxAbsScaler()\n",
    ">>> X_train_maxabs = max_abs_scaler.fit_transform(X_train)\n",
    ">>> print(X_train_maxabs)\n",
    ">>> X_test = np.array([[ -3., -1.,  4.]])\n",
    ">>> X_test_maxabs = max_abs_scaler.transform(X_test)\n",
    ">>> print(X_test_maxabs)\n",
    ">>> max_abs_scaler.scale_         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`robust_scale/RobustScaler`- data contains many outliers, scaling using the mean and variance of the data is likely to not work very well, They use more robust estimates for the center and range of your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-Linear Transformation  \n",
    "`QuantileTransformer/quantile_transform` provide a non-parametric transformation based on the quantile function to map the data to a uniform distribution with values between 0 and 1  \n",
    "It is also possible to map the transformed data to a normal distribution by setting output_distribution='normal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.3 5.1 5.8 6.5 7.9]\n",
      "[[4.3        2.         1.         0.1       ]\n",
      " [4.31491491 2.02982983 1.01491491 0.1       ]\n",
      " [4.32982983 2.05965966 1.02982983 0.1       ]\n",
      " ...\n",
      " [7.84034034 4.34034034 6.84034034 2.5       ]\n",
      " [7.87017017 4.37017017 6.87017017 2.5       ]\n",
      " [7.9        4.4        6.9        2.5       ]]\n"
     ]
    }
   ],
   "source": [
    ">>> from sklearn.datasets import load_iris\n",
    ">>> from sklearn.model_selection import train_test_split\n",
    ">>> iris = load_iris()\n",
    ">>> X, y = iris.data, iris.target\n",
    ">>> X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    ">>> quantile_transformer = QuantileTransformer(random_state=0)\n",
    ">>> X_train_trans = quantile_transformer.fit_transform(X_train)\n",
    ">>> X_test_trans = quantile_transformer.transform(X_test)\n",
    ">>> print(np.percentile(X_train[:, 0], [0, 25, 50, 75, 100]))\n",
    ">>> quantile_transformer = QuantileTransformer(\n",
    "...     output_distribution='normal', random_state=0)\n",
    ">>> X_trans = quantile_transformer.fit_transform(X)\n",
    ">>> print(quantile_transformer.quantiles_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization  \n",
    "Normalization is the process of scaling individual samples to have unit norm  \n",
    "This process can be useful if you plan to use a quadratic form such as the dot-product or any other kernel to quantify the similarity of any pair of samples  \n",
    "`normalize/Normalizer(scioy.sparse-liked-input)` - provides a quick and easy way to perform this operation on a single array-like dataset, either using the l1 or l2 norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.40824829 -0.40824829  0.81649658]\n",
      " [ 1.          0.          0.        ]\n",
      " [ 0.          0.70710678 -0.70710678]]\n",
      "[[ 0.40824829 -0.40824829  0.81649658]\n",
      " [ 1.          0.          0.        ]\n",
      " [ 0.          0.70710678 -0.70710678]]\n",
      "[[-0.70710678  0.70710678  0.        ]]\n"
     ]
    }
   ],
   "source": [
    ">>> X = [[ 1., -1.,  2.],[ 2.,  0.,  0.],[ 0.,  1., -1.]]\n",
    ">>> X_normalized = sk_normalize(X, norm='l2')\n",
    ">>> print(X_normalized)\n",
    ">>> normalizer = Normalizer().fit(X)  # fit does nothing\n",
    ">>> print(normalizer.transform(X))\n",
    ">>> print(normalizer.transform([[-1.,  1., 0.]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binarization  \n",
    "Feature binarization is the process of thresholding numerical features to get boolean values  \n",
    "This can be useful for downstream probabilistic estimators that make assumption that the input data is distributed according to a multi-variate Bernoulli distribution  \n",
    "It is also common among the text processing community to use binary feature values (probably to simplify the probabilistic reasoning) even if normalized counts (a.k.a. term frequencies) or TF-IDF valued features often perform slightly better in practice.  \n",
    "`Binarizer(threshold)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "[[0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    ">>> X = [[ 1., -1.,  2.],\n",
    "...      [ 2.,  0.,  0.],\n",
    "...      [ 0.,  1., -1.]]\n",
    "\n",
    ">>> binarizer = Binarizer().fit(X)  # fit does nothing\n",
    ">>> print(binarizer.transform(X))\n",
    ">>> binarizer = Binarizer(threshold=1.1)\n",
    ">>> print(binarizer.transform(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding Categorical Features  \n",
    "Convert categorical features to features that can be used with scikit-learn estimators\n",
    " `OneHotEncoder` - implemented the `one-of-K/one-hot` encoding,  transforms each categorical feature with m possible values into m binary features, with only one active( if there is a possibility that the training data might have missing categorical features, one has to explicitly set n_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 1. 0. 0. 0. 0. 1.]]\n",
      "OneHotEncoder(categorical_features='all', dtype=<class 'numpy.float64'>,\n",
      "       handle_unknown='error', n_values=[2, 3, 4], sparse=True)\n"
     ]
    }
   ],
   "source": [
    ">>> enc = OneHotEncoder()\n",
    ">>> enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])\n",
    ">>> print(enc.transform([[0, 1, 3]]).toarray())\n",
    ">>> enc = OneHotEncoder(n_values=[2, 3, 4])\n",
    ">>> # Note that there are missing categorical values for the 2nd and 3rd\n",
    ">>> # features\n",
    ">>> print(enc.fit([[1, 2, 3], [0, 2, 0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation(填充)  \n",
    "How to handle missing value in dataset?  \n",
    "\n",
    "1. Discard entire rows and/or columns containing missing values  \n",
    "2. Impute the missing values(i.e., to infer them from the known part of the data)  \n",
    "\n",
    "`Imputer` - provides basic strategies for imputing missing values, either using the mean, the median or the most frequent value of the row or column in which the missing values are located, this class also allows for different missing values encodings, support sparse matrices    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.         2.        ]\n",
      " [6.         3.66666667]\n",
      " [7.         6.        ]]\n",
      "[[4.         2.        ]\n",
      " [6.         3.66666667]\n",
      " [7.         6.        ]]\n"
     ]
    }
   ],
   "source": [
    ">>> import numpy as np\n",
    ">>> from sklearn.preprocessing import Imputer\n",
    ">>> imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    ">>> imp.fit([[1, 2], [np.nan, 3], [7, 6]])\n",
    ">>> X = [[np.nan, 2], [6, np.nan], [7, 6]]\n",
    ">>> print(imp.transform(X))  \n",
    ">>> import scipy.sparse as sp\n",
    ">>> X = sp.csc_matrix([[1, 2], [0, 3], [7, 6]])\n",
    ">>> imp = Imputer(missing_values=0, strategy='mean', axis=0)\n",
    ">>> imp.fit(X)\n",
    ">>> X_test = sp.csc_matrix([[0, 2], [6, 0], [7, 6]])\n",
    ">>> print(imp.transform(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating Polynomial Features  \n",
    "Often it’s useful to add complexity to the model by considering nonlinear features of the input data. A simple and common method to use is polynomial features, which can get features’ high-order and interaction terms  \n",
    "`PolynomialFeatures`  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1]\n",
      " [2 3]\n",
      " [4 5]]\n",
      "[[ 1.  0.  1.  0.  0.  1.]\n",
      " [ 1.  2.  3.  4.  6.  9.]\n",
      " [ 1.  4.  5. 16. 20. 25.]]\n",
      "[[0 1 2]\n",
      " [3 4 5]\n",
      " [6 7 8]]\n",
      "[[  1.   0.   1.   2.   0.   0.   2.   0.]\n",
      " [  1.   3.   4.   5.  12.  15.  20.  60.]\n",
      " [  1.   6.   7.   8.  42.  48.  56. 336.]]\n"
     ]
    }
   ],
   "source": [
    ">>> import numpy as np\n",
    ">>> from sklearn.preprocessing import PolynomialFeatures\n",
    ">>> X = np.arange(6).reshape(3, 2)\n",
    ">>> print(X)\n",
    ">>> poly = PolynomialFeatures(2)\n",
    ">>> print(poly.fit_transform(X))\n",
    ">>> X = np.arange(9).reshape(3, 3)\n",
    ">>> print(X)\n",
    ">>> poly = PolynomialFeatures(degree=3, interaction_only=True)\n",
    ">>> print(poly.fit_transform(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Transformer  \n",
    "`FunctionTransformer` - Convert an existing Python function into a transformer to assist in data cleaning or processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.69314718]\n",
      " [1.09861229 1.38629436]]\n"
     ]
    }
   ],
   "source": [
    ">>> import numpy as np\n",
    ">>> from sklearn.preprocessing import FunctionTransformer\n",
    ">>> transformer = FunctionTransformer(np.log1p)\n",
    ">>> X = np.array([[0, 1], [2, 3]])\n",
    ">>> print(transformer.transform(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UnSupervised Dimensionality Reduction  \n",
    "[Detail](http://scikit-learn.org/stable/modules/unsupervised_reduction.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Projection  \n",
    "Reduce the dimensionality of the data by trading a controlled amount of accuracy, for faster processing times and smaller model sizes.\n",
    "\n",
    "#### The Johnson-Lindenstrauss Lemma  \n",
    "The main theoretical result behind the efficiency of random projection is the Johnson-Lindenstrauss lemma (quoting Wikipedia):\n",
    "> In mathematics, the Johnson-Lindenstrauss lemma is a result concerning low-distortion embeddings of points from high-dimensional into low-dimensional Euclidean space. The lemma states that a small set of points in a high-dimensional space can be embedded into a space of much lower dimension in such a way that distances between the points are nearly preserved. The map used for the embedding is at least Lipschitz, and can even be taken to be an orthogonal projection.   \n",
    "\n",
    "`johnson_lindenstrauss_min_dim` - Knowing only the number of sample, estimates conservatively the minimal size of the random subspace to guarantee a bounded distortion introduced by the random projection  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "663\n",
      "[    663   11841 1112658]\n",
      "[ 7894  9868 11841]\n"
     ]
    }
   ],
   "source": [
    ">>> from sklearn.random_projection import johnson_lindenstrauss_min_dim\n",
    ">>> print(johnson_lindenstrauss_min_dim(n_samples=1e6, eps=0.5))\n",
    ">>> print(johnson_lindenstrauss_min_dim(n_samples=1e6, eps=[0.5, 0.1, 0.01]))\n",
    ">>> print(johnson_lindenstrauss_min_dim(n_samples=[1e4, 1e5, 1e6], eps=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian Random Projection  \n",
    "`GaussianRandomProjection` - reduces the dimensionality by projecting the original input space on a randomly generated matrix where components are drawn from the following distribution N(0, \\frac{1}{n_{components}})  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 3947)\n"
     ]
    }
   ],
   "source": [
    ">>> import numpy as np\n",
    ">>> from sklearn import random_projection\n",
    ">>> X = np.random.rand(100, 10000)\n",
    ">>> transformer = random_projection.GaussianRandomProjection()\n",
    ">>> X_new = transformer.fit_transform(X)\n",
    ">>> print(X_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sparse Random Projection  \n",
    "`SparseRandomProjection` - reduces the dimensionality by projecting the original input space using a sparse random matrix  \n",
    "[Detail](http://scikit-learn.org/stable/modules/random_projection.html#sparse-random-projection)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 3947)\n"
     ]
    }
   ],
   "source": [
    ">>> import numpy as np\n",
    ">>> from sklearn import random_projection\n",
    ">>> X = np.random.rand(100,10000)\n",
    ">>> transformer = random_projection.SparseRandomProjection()\n",
    ">>> X_new = transformer.fit_transform(X)\n",
    ">>> print(X_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Approximation  \n",
    "[Detail](http://scikit-learn.org/stable/modules/kernel_approximation.html)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise Metrics, Affinities, Kernels  \n",
    "evaluate pairwise distances or affinity of sets of samples(distance metrics and kernels)  \n",
    "all following functions under `sklearn.metrics.pairwise`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cosine Similarity  \n",
    "`cosine_similarity(sparse)`  \n",
    "cosine_similarity computes the L2-normalized dot product of vectors. That is, if x and y are row vectors, their cosine similarity k is defined as:\n",
    "\n",
    "k(x, y) = \\frac{x y^\\top}{\\|x\\| \\|y\\|}\n",
    "\n",
    "This is called cosine similarity, because Euclidean (L2) normalization projects the vectors onto the unit sphere, and their dot product is then the cosine of the angle between the points denoted by the vectors.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Kernel  \n",
    "`linear_kernel` - computes the linear kernel, that is, a special case of polynomial_kernel with degree=1 and coef0=0 (homogeneous). If x and y are column vectors, their linear kernel is:\n",
    "\n",
    "k(x, y) = x^\\top y  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polynomial kernel  \n",
    "`polynomial_kernel` - computes the degree-d polynomial kernel between two vectors. The polynomial kernel represents the similarity between two vectors. Conceptually, the polynomial kernels considers not only the similarity between vectors under the same dimension, but also across dimensions. When used in machine learning algorithms, this allows to account for feature interaction.\n",
    "\n",
    "The polynomial kernel is defined as:\n",
    "\n",
    "k(x, y) = (\\gamma x^\\top y +c_0)^d\n",
    "\n",
    "where:\n",
    "\n",
    "x, y are the input vectors\n",
    "d is the kernel degree\n",
    "If c_0 = 0 the kernel is said to be homogeneous.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sigmoid Kernel  \n",
    "`sigmoid_kernel` computes the sigmoid kernel between two vectors. The sigmoid kernel is also known as hyperbolic tangent, or Multilayer Perceptron (because, in the neural network field, it is often used as neuron activation function). It is defined as:\n",
    "\n",
    "k(x, y) = \\tanh( \\gamma x^\\top y + c_0)\n",
    "\n",
    "where:\n",
    "\n",
    "x, y are the input vectors\n",
    "\\gamma is known as slope\n",
    "c_0 is known as intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RBF Kernel  \n",
    "`rbf_kernel` computes the radial basis function (RBF) kernel between two vectors. This kernel is defined as:\n",
    "\n",
    "k(x, y) = \\exp( -\\gamma \\| x-y \\|^2)\n",
    "\n",
    "where x and y are the input vectors. If \\gamma = \\sigma^{-2} the kernel is known as the Gaussian kernel of variance \\sigma^2.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Laplacian Kernel  \n",
    "`laplacian_kernel` is a variant on the radial basis function kernel defined as:\n",
    "\n",
    "k(x, y) = \\exp( -\\gamma \\| x-y \\|_1)\n",
    "\n",
    "where x and y are the input vectors and \\|x-y\\|_1 is the Manhattan distance between the input vectors.\n",
    "\n",
    "It has proven useful in ML applied to noiseless data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chi-squared kernel  \n",
    "The chi-squared kernel is a very popular choice for training non-linear SVMs in computer vision applications. It can be computed using chi2_kernel and then passed to an sklearn.svm.SVC with kernel=\"precomputed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.36787944 0.89483932 0.58364548]\n",
      " [0.36787944 1.         0.51341712 0.83822343]\n",
      " [0.89483932 0.51341712 1.         0.7768366 ]\n",
      " [0.58364548 0.83822343 0.7768366  1.        ]]\n",
      "[0 1 0 1]\n",
      "or\n",
      "[0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    ">>> from sklearn.svm import SVC\n",
    ">>> from sklearn.metrics.pairwise import chi2_kernel\n",
    ">>> X = [[0, 1], [1, 0], [.2, .8], [.7, .3]]\n",
    ">>> y = [0, 1, 0, 1]\n",
    ">>> K = chi2_kernel(X, gamma=.5)\n",
    ">>> print(K)\n",
    ">>> svm = SVC(kernel='precomputed').fit(K, y)\n",
    ">>> print(svm.predict(K))\n",
    ">>> print('or')\n",
    ">>> svm = SVC(kernel=chi2_kernel).fit(X, y)\n",
    ">>> print(svm.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming The Prediction Target(y)  \n",
    "#### LabelBinarizer  \n",
    "create a label indicator matrix from a list of multi-class labels  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)\n",
      "[1 2 4 6]\n",
      "[[1 0 0 0]\n",
      " [0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    ">>> from sklearn import preprocessing\n",
    ">>> lb = preprocessing.LabelBinarizer()\n",
    ">>> print(lb.fit([1, 2, 6, 4, 2]))\n",
    ">>> print(lb.classes_)\n",
    ">>> print(lb.transform([1, 6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LabelEncoder  \n",
    "normalize labels such that they contain only values between 0 and n_classes-1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 6]\n",
      "[0 0 1 2]\n",
      "[1 1 2 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jferroal/.venvs/lite_data_science/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    ">>> from sklearn import preprocessing\n",
    ">>> le = preprocessing.LabelEncoder()\n",
    ">>> le.fit([1, 2, 2, 6])\n",
    ">>> print(le.classes_)\n",
    ">>> print(le.transform([1, 1, 2, 6]))\n",
    ">>> print(le.inverse_transform([0, 0, 1, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can also be used to transform non-numerical labels (as long as they are hashable and comparable) to numerical labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amsterdam', 'paris', 'tokyo']\n",
      "[2 2 1]\n",
      "['tokyo', 'tokyo', 'paris']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jferroal/.venvs/lite_data_science/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    ">>> le = preprocessing.LabelEncoder()\n",
    ">>> le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"])\n",
    ">>> print(list(le.classes_))\n",
    ">>> print(le.transform([\"tokyo\", \"tokyo\", \"paris\"]))\n",
    ">>> print(list(le.inverse_transform([2, 2, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning  \n",
    "### Generalized Linear Models  \n",
    "The following are a set of methods intended for regression in which the target value is expected to be a linear combination of the input variables. In mathematical notion, if \\hat{y} is the predicted value.\n",
    "\n",
    "\\hat{y}(w, x) = w_0 + w_1 x_1 + ... + w_p x_p\n",
    "\n",
    "Across the module, we designate the vector w = (w_1,\n",
    "..., w_p) as coef_ and w_0 as intercept_.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LinearRegression  \n",
    "Mathematically it solves a problem of the form:\n",
    "\n",
    "\\underset{w}{min\\,} {|| X w - y||_2}^2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV,Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 0.5])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> reg = LinearRegression()\n",
    ">>> reg.fit ([[0, 0], [1, 1], [2, 2]], [0, 1, 2])\n",
    ">>> reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge Regression  \n",
    "The ridge coefficients minimize a penalized residual sum of squares,\n",
    "\n",
    "\\underset{w}{min\\,} {{|| X w - y||_2}^2 + \\alpha {||w||_2}^2}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.34545455 0.34545455]\n",
      "0.13636363636363638\n"
     ]
    }
   ],
   "source": [
    ">>> reg = Ridge (alpha = .5)\n",
    ">>> reg.fit ([[0, 0], [0, 0], [1, 1]], [0, .1, 1]) \n",
    ">>> print(reg.coef_)\n",
    ">>> print(reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge Cross Validation  \n",
    "RidgeCV implements ridge regression with built-in cross-validation of the alpha parameter. The object works in the same way as GridSearchCV except that it defaults to Generalized Cross-Validation (GCV), an efficient form of leave-one-out cross-validation  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> reg = RidgeCV(alphas=[0.1, 1.0, 10.0])\n",
    ">>> reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])       \n",
    ">>> reg.alpha_                                      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso  \n",
    "The Lasso is a linear model that estimates sparse coefficients  \n",
    "It is useful in some contexts due to its tendency to prefer solutions with fewer parameter values, effectively reducing the number of variables upon which the given solution is dependent. For this reason, the Lasso and its variants are fundamental to the field of compressed sensing. Under certain conditions, it can recover the exact set of non-zero weights  \n",
    "Mathematically, it consists of a linear model trained with \\ell_1 prior as regularizer. The objective function to minimize is:\n",
    "\n",
    "\\underset{w}{min\\,} { \\frac{1}{2n_{samples}} ||X w - y||_2 ^ 2 + \\alpha ||w||_1}\n",
    "\n",
    "The lasso estimate thus solves the minimization of the least-squares penalty with \\alpha ||w||_1 added, where \\alpha is a constant and ||w||_1 is the \\ell_1-norm of the parameter vector.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> reg = Lasso(alpha = 0.1)\n",
    ">>> reg.fit([[0, 0], [1, 1]], [0, 1])\n",
    ">>> reg.predict([[1, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[More](http://scikit-learn.org/stable/modules/linear_model.html#setting-regularization-parameter)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Task Lasso  \n",
    "The MultiTaskLasso is a linear model that estimates sparse coefficients for multiple regression problems jointly: y is a 2D array, of shape (n_samples, n_tasks). The constraint is that the selected features are the same for all the regression problems, also called tasks  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elastic Net  \n",
    "ElasticNet is a linear regression model trained with L1 and L2 prior as regularizer. This combination allows for learning a sparse model where few of the weights are non-zero like Lasso, while still maintaining the regularization properties of Ridge. We control the convex combination of L1 and L2 using the l1_ratio parameter.  \n",
    "Elastic-net is useful when there are multiple features which are correlated with one another. Lasso is likely to pick one of these at random, while elastic-net is likely to pick both.\n",
    "\n",
    "A practical advantage of trading-off between Lasso and Ridge is it allows Elastic-Net to inherit some of Ridge’s stability under rotation.\n",
    "\n",
    "The objective function to minimize is in this case\n",
    "\n",
    "\\underset{w}{min\\,} { \\frac{1}{2n_{samples}} ||X w - y||_2 ^ 2 + \\alpha \\rho ||w||_1 +\n",
    "\\frac{\\alpha(1-\\rho)}{2} ||w||_2 ^ 2}  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-task Elastic Net  \n",
    "The MultiTaskElasticNet is an elastic-net model that estimates sparse coefficients for multiple regression problems jointly: Y is a 2D array, of shape (n_samples, n_tasks). The constraint is that the selected features are the same for all the regression problems, also called tasks.\n",
    "\n",
    "Mathematically, it consists of a linear model trained with a mixed \\ell_1 \\ell_2 prior and \\ell_2 prior as regularizer. The objective function to minimize is:\n",
    "\n",
    "\\underset{W}{min\\,} { \\frac{1}{2n_{samples}} ||X W - Y||_{Fro}^2 + \\alpha \\rho ||W||_{2 1} +\n",
    "\\frac{\\alpha(1-\\rho)}{2} ||W||_{Fro}^2}  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Least Angle Regression  \n",
    "Least-angle regression (LARS) is a regression algorithm for high-dimensional data  \n",
    "LARS is similar to forward stepwise regression. At each step, it finds the predictor most correlated with the response. When there are multiple predictors having equal correlation, instead of continuing along the same predictor, it proceeds in a direction equiangular between the predictors.  \n",
    "\n",
    "The advantages of LARS are:\n",
    "1. It is numerically efficient in contexts where p >> n (i.e., when the number of dimensions is significantly greater than the number of points)\n",
    "2. It is computationally just as fast as forward selection and has the same order of complexity as an ordinary least squares.\n",
    "3. It produces a full piecewise linear solution path, which is useful in cross-validation or similar attempts to tune the model.\n",
    "4. If two variables are almost equally correlated with the response, then their coefficients should increase at approximately the same rate. The algorithm thus behaves as intuition would expect, and also is more stable.\n",
    "5. It is easily modified to produce solutions for other estimators, like the Lasso.  \n",
    "\n",
    "The disadvantages of the LARS method include:\n",
    "\n",
    "1. Because LARS is based upon an iterative refitting of the residuals, it would appear to be especially sensitive to the effects of noise. This problem is discussed in detail by Weisberg in the discussion section of the Efron et al. (2004) Annals of Statistics article.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LARS Lasso  \n",
    "LassoLars is a lasso model implemented using the LARS algorithm, and unlike the implementation based on coordinate_descent, this yields the exact solution, which is piecewise linear as a function of the norm of its coefficients.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Orthogonal Matching Pursuit  \n",
    "OrthogonalMatchingPursuit and orthogonal_mp implements the OMP algorithm for approximating the fit of a linear model with constraints imposed on the number of non-zero coefficients (ie. the L 0 pseudo-norm).\n",
    "\n",
    "Being a forward feature selection method like Least Angle Regression, orthogonal matching pursuit can approximate the optimum solution vector with a fixed number of non-zero elements:\n",
    "\n",
    "\\text{arg\\,min\\,} ||y - X\\gamma||_2^2 \\text{ subject to } \\\n",
    "||\\gamma||_0 \\leq n_{nonzero\\_coefs}\n",
    "\n",
    "Alternatively, orthogonal matching pursuit can target a specific error instead of a specific number of non-zero coefficients. This can be expressed as:\n",
    "\n",
    "\\text{arg\\,min\\,} ||\\gamma||_0 \\text{ subject to } ||y-X\\gamma||_2^2 \\\n",
    "\\leq \\text{tol}\n",
    "\n",
    "OMP is based on a greedy algorithm that includes at each step the atom most highly correlated with the current residual. It is similar to the simpler matching pursuit (MP) method, but better in that at each iteration, the residual is recomputed using an orthogonal projection on the space of the previously chosen dictionary elements.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayesian Regression  \n",
    "Bayesian regression techniques can be used to include regularization parameters in the estimation procedure: the regularization parameter is not set in a hard sense but tuned to the data at hand.\n",
    "\n",
    "This can be done by introducing uninformative priors over the hyper parameters of the model. The \\ell_{2} regularization used in Ridge Regression is equivalent to finding a maximum a posteriori estimation under a Gaussian prior over the parameters w with precision \\lambda^{-1}. Instead of setting lambda manually, it is possible to treat it as a random variable to be estimated from the data.\n",
    "\n",
    "To obtain a fully probabilistic model, the output y is assumed to be Gaussian distributed around X w:\n",
    "\n",
    "p(y|X,w,\\alpha) = \\mathcal{N}(y|X w,\\alpha)\n",
    "\n",
    "Alpha is again treated as a random variable that is to be estimated from the data.\n",
    "\n",
    "The advantages of Bayesian Regression are:\n",
    "\n",
    "1. It adapts to the data at hand.\n",
    "2. It can be used to include regularization parameters in the estimation procedure.  \n",
    "\n",
    "The disadvantages of Bayesian regression include:\n",
    "\n",
    "1. Inference of the model can be time consuming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Bayesian Ridge Regression  \n",
    "BayesianRidge estimates a probabilistic model of the regression problem as described above. The prior for the parameter w is given by a spherical Gaussian:\n",
    "\n",
    "p(w|\\lambda) =\n",
    "\\mathcal{N}(w|0,\\lambda^{-1}\\bold{I_{p}})\n",
    "\n",
    "The priors over \\alpha and \\lambda are chosen to be gamma distributions, the conjugate prior for the precision of the Gaussian.\n",
    "\n",
    "The resulting model is called Bayesian Ridge Regression, and is similar to the classical Ridge. The parameters w, \\alpha and \\lambda are estimated jointly during the fit of the model. The remaining hyperparameters are the parameters of the gamma priors over \\alpha and \\lambda. These are usually chosen to be non-informative. The parameters are estimated by maximizing the marginal log likelihood.\n",
    "\n",
    "By default \\alpha_1 = \\alpha_2 =  \\lambda_1 = \\lambda_2 = 10^{-6}.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automatic Relevance Determination   \n",
    "ARDRegression is very similar to Bayesian Ridge Regression, but can lead to sparser weights w [1] [2]. ARDRegression poses a different prior over w, by dropping the assumption of the Gaussian being spherical.\n",
    "\n",
    "Instead, the distribution over w is assumed to be an axis-parallel, elliptical Gaussian distribution.\n",
    "\n",
    "This means each weight w_{i} is drawn from a Gaussian distribution, centered on zero and with a precision \\lambda_{i}:\n",
    "\n",
    "p(w|\\lambda) = \\mathcal{N}(w|0,A^{-1})\n",
    "\n",
    "with diag \\; (A) = \\lambda = \\{\\lambda_{1},...,\\lambda_{p}\\}.\n",
    "\n",
    "In contrast to Bayesian Ridge Regression, each coordinate of w_{i} has its own standard deviation \\lambda_i. The prior over all \\lambda_i is chosen to be the same gamma distribution given by hyperparameters \\lambda_1 and \\lambda_2.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression  \n",
    "Logistic regression, despite its name, is a linear model for classification rather than regression. Logistic regression is also known in the literature as logit regression, maximum-entropy classification (MaxEnt) or the log-linear classifier. In this model, the probabilities describing the possible outcomes of a single trial are modeled using a logistic function.\n",
    "\n",
    "The implementation of logistic regression in scikit-learn can be accessed from class LogisticRegression. This implementation can fit binary, One-vs- Rest, or multinomial logistic regression with optional L2 or L1 regularization.\n",
    "\n",
    "As an optimization problem, binary class L2 penalized logistic regression minimizes the following cost function:\n",
    "\n",
    "\\underset{w, c}{min\\,} \\frac{1}{2}w^T w + C \\sum_{i=1}^n \\log(\\exp(- y_i (X_i^T w + c)) + 1) .\n",
    "\n",
    "Similarly, L1 regularized logistic regression solves the following optimization problem\n",
    "\n",
    "\\underset{w, c}{min\\,} \\|w\\|_1 + C \\sum_{i=1}^n \\log(\\exp(- y_i (X_i^T w + c)) + 1) .\n",
    "\n",
    "The solvers implemented in the class LogisticRegression are “liblinear”, “newton-cg”, “lbfgs”, “sag” and “saga”:\n",
    "\n",
    "The solver “liblinear” uses a coordinate descent (CD) algorithm, and relies on the excellent C++ LIBLINEAR library, which is shipped with scikit-learn. However, the CD algorithm implemented in liblinear cannot learn a true multinomial (multiclass) model; instead, the optimization problem is decomposed in a “one-vs-rest” fashion so separate binary classifiers are trained for all classes. This happens under the hood, so LogisticRegression instances using this solver behave as multiclass classifiers. For L1 penalization sklearn.svm.l1_min_c allows to calculate the lower bound for C in order to get a non “null” (all feature weights to zero) model.\n",
    "\n",
    "The “lbfgs”, “sag” and “newton-cg” solvers only support L2 penalization and are found to converge faster for some high dimensional data. Setting multi_class to “multinomial” with these solvers learns a true multinomial logistic regression model [5], which means that its probability estimates should be better calibrated than the default “one-vs-rest” setting.\n",
    "\n",
    "The “sag” solver uses a Stochastic Average Gradient descent [6]. It is faster than other solvers for large datasets, when both the number of samples and the number of features are large.\n",
    "\n",
    "The “saga” solver [7] is a variant of “sag” that also supports the non-smooth penalty=”l1” option. This is therefore the solver of choice for sparse multinomial logistic regression.\n",
    "\n",
    "In a nutshell, one may choose the solver with the following rules:\n",
    "\n",
    "Case\tSolver\n",
    "L1 penalty\t“liblinear” or “saga”\n",
    "Multinomial loss\t“lbfgs”, “sag”, “saga” or “newton-cg”\n",
    "Very Large dataset (n_samples)\t“sag” or “saga”\n",
    "The “saga” solver is often the best choice. The “liblinear” solver is used by default for historical reasons.\n",
    "\n",
    "For large dataset, you may also consider using SGDClassifier with ‘log’ loss.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent  \n",
    "Stochastic gradient descent is a simple yet very efficient approach to fit linear models. It is particularly useful when the number of samples (and the number of features) is very large. The partial_fit method allows only/out-of-core learning.\n",
    "\n",
    "The classes SGDClassifier and SGDRegressor provide functionality to fit linear models for classification and regression using different (convex) loss functions and different penalties. E.g., with loss=\"log\", SGDClassifier fits a logistic regression model, while with loss=\"hinge\" it fits a linear support vector machine (SVM).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perceptron  \n",
    "he Perceptron is another simple algorithm suitable for large scale learning. By default:\n",
    "\n",
    "1. It does not require a learning rate.\n",
    "2. It is not regularized (penalized).\n",
    "3. It updates its model only on mistakes.  \n",
    "\n",
    "The last characteristic implies that the Perceptron is slightly faster to train than SGD with the hinge loss and that the resulting models are sparser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Passive Aggressive Algorithms  \n",
    "The passive-aggressive algorithms are a family of algorithms for large-scale learning. They are similar to the Perceptron in that they do not require a learning rate. However, contrary to the Perceptron, they include a regularization parameter C.\n",
    "\n",
    "For classification, PassiveAggressiveClassifier can be used with loss='hinge' (PA-I) or loss='squared_hinge' (PA-II). For regression, PassiveAggressiveRegressor can be used with loss='epsilon_insensitive' (PA-I) or loss='squared_epsilon_insensitive' (PA-II).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More  \n",
    "http://scikit-learn.org/stable/modules/linear_model.html#robustness-regression-outliers-and-modeling-errors  \n",
    "http://scikit-learn.org/stable/modules/linear_model.html#polynomial-regression-extending-linear-models-with-basis-functions  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear and Quadratic Discriminant Analysis\n",
    "[线性二次判别分析](http://scikit-learn.org/stable/modules/lda_qda.html#linear-and-quadratic-discriminant-analysis)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel ridge regression  \n",
    "http://scikit-learn.org/stable/modules/kernel_ridge.html#kernel-ridge-regression  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines  \n",
    "Support vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection.\n",
    "\n",
    "The advantages of support vector machines are:\n",
    "\n",
    "1. Effective in high dimensional spaces.  \n",
    "2. Still effective in cases where number of dimensions is greater than the number of samples.\n",
    "3. Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n",
    "4. Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.  \n",
    "\n",
    "The disadvantages of support vector machines include:\n",
    "\n",
    "1. If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.\n",
    "2. SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation (see Scores and probabilities, below).\n",
    "3. The support vector machines in scikit-learn support both dense (numpy.ndarray and convertible to that by numpy.asarray) and sparse (any scipy.sparse) sample vectors as input. However, to use an SVM to make predictions for sparse data, it must have been fit on such data. For optimal performance, use C-ordered numpy.ndarray (dense) or scipy.sparse.csr_matrix (sparse) with dtype=float64.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification  \n",
    "[More Detail](http://scikit-learn.org/stable/modules/svm.html#classification)  \n",
    "`svm.SVC/NuSVC/LinearSVC`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE: I Think I Should Not Read Model Now !!!!  \n",
    "**I am working [here](http://scikit-learn.org/stable/modules/svm.html)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Un-Supervised Learning  \n",
    "[Detail](http://scikit-learn.org/stable/unsupervised_learning.html)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection and Evaluation  \n",
    "### Cross-Validation: Evaluating Estimator Perfomance  \n",
    "Overfitting: Learning the parameters of a prediction function and testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data  \n",
    "\n",
    "#### Train Test Split  \n",
    "To avoid overfitting, it is common practice when performing a (supervised) machine learning experiment to hold out part of the available data as a test set X_test, y_test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 4) (90,)\n",
      "(60, 4) (60,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> import numpy as np\n",
    ">>> from sklearn.model_selection import train_test_split\n",
    ">>> from sklearn import datasets\n",
    ">>> from sklearn import svm\n",
    ">>> iris = datasets.load_iris()\n",
    ">>> iris.data.shape, iris.target.shape\n",
    ">>> X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.4, random_state=0)\n",
    ">>> print(X_train.shape, y_train.shape)\n",
    ">>> print(X_test.shape, y_test.shape)\n",
    ">>> clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\n",
    ">>> clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation  \n",
    "Partition the available data into three sets, we drastically reduce the number of samples which can be used for learning the model, and the results can depend on a particular random choice for the pair of (train, validation) sets.  \n",
    "A test set should still be held out for final evaluation, but the validation set is no longer needed when doing CV. In the basic approach, called k-fold CV, the training set is split into k smaller sets (other approaches are described below, but generally follow the same principles). The following procedure is followed for each of the k “folds”:\n",
    "\n",
    "A model is trained using k-1 of the folds as training data;\n",
    "the resulting model is validated on the remaining part of the data (i.e., it is used as a test set to compute a performance measure such as accuracy).\n",
    "The performance measure reported by k-fold cross-validation is then the average of the values computed in the loop. This approach can be computationally expensive, but does not waste too much data (as it is the case when fixing an arbitrary test set), which is a major advantage in problem such as inverse inference where the number of samples is very small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cross_val_score  \n",
    "The simplest way to use cross-validation  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.96666667 1.         0.96666667 0.96666667 1.        ]\n",
      "[0.96658312 1.         0.96658312 0.96658312 1.        ]\n",
      "[0.97777778 0.97777778 1.        ]\n"
     ]
    }
   ],
   "source": [
    ">>> from sklearn.model_selection import cross_val_score\n",
    ">>> clf = svm.SVC(kernel='linear', C=1)\n",
    ">>> scores = cross_val_score(clf, iris.data, iris.target, cv=5)\n",
    ">>> print(scores)\n",
    ">>> from sklearn import metrics\n",
    ">>> scores = cross_val_score(clf, iris.data, iris.target, cv=5, scoring='f1_macro')\n",
    ">>> print(scores)\n",
    ">>> from sklearn.model_selection import ShuffleSplit\n",
    ">>> n_samples = iris.data.shape[0]\n",
    ">>> cv = ShuffleSplit(n_splits=3, test_size=0.3, random_state=0)\n",
    ">>> scores = cross_val_score(clf, iris.data, iris.target, cv=cv)\n",
    ">>> print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cross_validate  \n",
    "The cross_validate function differs from cross_val_score in two ways -\n",
    "\n",
    "It allows specifying multiple metrics for evaluation.\n",
    "It returns a dict containing training scores, fit-times and score-times in addition to the test score.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([0.00059986, 0.00034595, 0.00042748]), 'score_time': array([0.00043631, 0.00042534, 0.0004766 ]), 'test_score': array([1.        , 0.96491228, 0.98039216]), 'train_score': array([0.98095238, 1.        , 0.99047619])}\n",
      "{'fit_time': array([0.0004909 , 0.00034809, 0.00028443, 0.00039887, 0.00032568]), 'score_time': array([0.00081444, 0.00071907, 0.00082541, 0.00102878, 0.00071669]), 'test_precision_macro': array([0.96969697, 1.        , 0.96969697, 0.96969697, 1.        ]), 'test_recall_macro': array([0.96666667, 1.        , 0.96666667, 0.96666667, 1.        ])}\n",
      "{'fit_time': array([0.00064826, 0.00041986, 0.00031424, 0.00035644, 0.00031781]), 'score_time': array([0.00111794, 0.00079131, 0.00077844, 0.00088596, 0.00078726]), 'test_prec_macro': array([0.96969697, 1.        , 0.96969697, 0.96969697, 1.        ]), 'train_prec_macro': array([0.97674419, 0.97674419, 0.99186992, 0.98412698, 0.98333333]), 'test_rec_micro': array([0.96666667, 1.        , 0.96666667, 0.96666667, 1.        ]), 'train_rec_micro': array([0.975     , 0.975     , 0.99166667, 0.98333333, 0.98333333])}\n"
     ]
    }
   ],
   "source": [
    ">>> from sklearn.model_selection import cross_validate\n",
    ">>> from sklearn.metrics import recall_score\n",
    ">>> from sklearn.metrics.scorer import make_scorer\n",
    ">>> scores = cross_validate(clf, iris.data, iris.target, scoring='precision_macro')\n",
    ">>> print(scores)\n",
    ">>> scoring = ['precision_macro', 'recall_macro']\n",
    ">>> clf = svm.SVC(kernel='linear', C=1, random_state=0)\n",
    ">>> scores = cross_validate(clf, iris.data, iris.target, scoring=scoring, cv=5, return_train_score=False)\n",
    ">>> print(scores)                       \n",
    ">>> scoring = {'prec_macro': 'precision_macro', 'rec_micro': make_scorer(recall_score, average='macro')}\n",
    ">>> scores = cross_validate(clf, iris.data, iris.target, scoring=scoring, cv=5, return_train_score=True)\n",
    ">>> print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cross_val_predict  \n",
    "The function cross_val_predict has a similar interface to cross_val_score, but returns, for each element in the input, the prediction that was obtained for that element when it was in the test set. Only cross-validation strategies that assign all elements to a test set exactly once can be used  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9733333333333334"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> from sklearn.model_selection import cross_val_predict\n",
    ">>> predicted = cross_val_predict(clf, iris.data, iris.target, cv=10)\n",
    ">>> metrics.accuracy_score(iris.target, predicted) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation Iterators: KFold \n",
    " divides all the samples in k groups of samples, called folds (if k = n, this is equivalent to the Leave One Out strategy), of equal sizes (if possible). The prediction function is learned using k - 1 folds, and the fold left out is used for test.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3] [0 1]\n",
      "[0 1] [2 3]\n"
     ]
    }
   ],
   "source": [
    ">>> import numpy as np\n",
    ">>> from sklearn.model_selection import KFold\n",
    ">>> X = [\"a\", \"b\", \"c\", \"d\"]\n",
    ">>> kf = KFold(n_splits=2)\n",
    ">>> for train, test in kf.split(X): print(\"%s %s\" % (train, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation Iterators: RepeatedKFold & RepeatedStratifiedKFold  \n",
    "repeats K-Fold n times. It can be used when one requires to run KFold n times, producing different splits in each repetition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3] [0 1]\n",
      "[0 1] [2 3]\n",
      "[0 2] [1 3]\n",
      "[1 3] [0 2]\n"
     ]
    }
   ],
   "source": [
    ">>> import numpy as np\n",
    ">>> from sklearn.model_selection import RepeatedKFold\n",
    ">>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
    ">>> random_state = 12883823\n",
    ">>> rkf = RepeatedKFold(n_splits=2, n_repeats=2, random_state=random_state)\n",
    ">>> for train, test in rkf.split(X): print(\"%s %s\" % (train, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation Iterators: Leave P Out  \n",
    "LeavePOut is very similar to LeaveOneOut as it creates all the possible training/test sets by removing p samples from the complete set. For n samples, this produces {n \\choose p} train-test pairs. Unlike LeaveOneOut and KFold, the test sets will overlap for p > 1.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3] [0 1]\n",
      "[1 3] [0 2]\n",
      "[1 2] [0 3]\n",
      "[0 3] [1 2]\n",
      "[0 2] [1 3]\n",
      "[0 1] [2 3]\n"
     ]
    }
   ],
   "source": [
    ">>> from sklearn.model_selection import LeavePOut\n",
    ">>> X = np.ones(4)\n",
    ">>> lpo = LeavePOut(p=2)\n",
    ">>> for train, test in lpo.split(X): print(\"%s %s\" % (train, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation Iterators: ShuffleSplit  \n",
    "The ShuffleSplit iterator will generate a user defined number of independent train / test dataset splits. Samples are first shuffled and then split into a pair of train and test sets.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 3 4] [2 0]\n",
      "[1 4 3] [0 2]\n",
      "[4 0 2] [1 3]\n"
     ]
    }
   ],
   "source": [
    ">>> from sklearn.model_selection import ShuffleSplit\n",
    ">>> X = np.arange(5)\n",
    ">>> ss = ShuffleSplit(n_splits=3, test_size=0.25, random_state=0)\n",
    ">>> for train_index, test_index in ss.split(X): print(\"%s %s\" % (train_index, test_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation Iterators: StratifiedKFold/RepeatedStratifiedKFold/StratifiedShuffleSplit  \n",
    "StratifiedKFold is a variation of k-fold which returns stratified folds: each set contains approximately the same percentage of samples of each target class as the complete set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 6 7 8 9] [0 1 4 5]\n",
      "[0 1 3 4 5 8 9] [2 6 7]\n",
      "[0 1 2 4 5 6 7] [3 8 9]\n"
     ]
    }
   ],
   "source": [
    ">>> from sklearn.model_selection import StratifiedKFold\n",
    ">>> X = np.ones(10)\n",
    ">>> y = [0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n",
    ">>> skf = StratifiedKFold(n_splits=3)\n",
    ">>> for train, test in skf.split(X, y): print(\"%s %s\" % (train, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation Iterators: GroupKFold  \n",
    "GroupKFold is a variation of k-fold which ensures that the same group is not represented in both testing and training set  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5] [6 7 8 9]\n",
      "[0 1 2 6 7 8 9] [3 4 5]\n",
      "[3 4 5 6 7 8 9] [0 1 2]\n"
     ]
    }
   ],
   "source": [
    ">>> from sklearn.model_selection import GroupKFold\n",
    ">>> X = [0.1, 0.2, 2.2, 2.4, 2.3, 4.55, 5.8, 8.8, 9, 10]\n",
    ">>> y = [\"a\", \"b\", \"b\", \"b\", \"c\", \"c\", \"c\", \"d\", \"d\", \"d\"]\n",
    ">>> groups = [1, 1, 1, 2, 2, 2, 3, 3, 3, 3]\n",
    ">>> gkf = GroupKFold(n_splits=3)\n",
    ">>> for train, test in gkf.split(X, y, groups=groups): print(\"%s %s\" % (train, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation Iterators: LeaveOneGroupOut/LeaveOneGroupOut    \n",
    "LeaveOneGroupOut is a cross-validation scheme which holds out the samples according to a third-party provided array of integer groups. This group information can be used to encode arbitrary domain specific pre-defined cross-validation folds.\n",
    "\n",
    "Each training set is thus constituted by all the samples except the ones related to a specific group.  \n",
    "\n",
    "LeavePGroupsOut is similar as LeaveOneGroupOut, but removes samples related to P groups for each training/test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 4 5 6] [0 1]\n",
      "[0 1 4 5 6] [2 3]\n",
      "[0 1 2 3] [4 5 6]\n",
      "[4 5] [0 1 2 3]\n",
      "[2 3] [0 1 4 5]\n",
      "[0 1] [2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    ">>> from sklearn.model_selection import LeaveOneGroupOut\n",
    ">>> X = [1, 5, 10, 50, 60, 70, 80]\n",
    ">>> y = [0, 1, 1, 2, 2, 2, 2]\n",
    ">>> groups = [1, 1, 2, 2, 3, 3, 3]\n",
    ">>> logo = LeaveOneGroupOut()\n",
    ">>> for train, test in logo.split(X, y, groups=groups): print(\"%s %s\" % (train, test))\n",
    ">>> from sklearn.model_selection import LeavePGroupsOut\n",
    ">>> X = np.arange(6)\n",
    ">>> y = [1, 1, 1, 2, 2, 2]\n",
    ">>> groups = [1, 1, 2, 2, 3, 3]\n",
    ">>> lpgo = LeavePGroupsOut(n_groups=2)\n",
    ">>> for train, test in lpgo.split(X, y, groups=groups): print(\"%s %s\" % (train, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation Iterators: GroupShuffleSplit  \n",
    "The GroupShuffleSplit iterator behaves as a combination of ShuffleSplit and LeavePGroupsOut, and generates a sequence of randomized partitions in which a subset of groups are held out for each split.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3] [4 5 6 7]\n",
      "[2 3 6 7] [0 1 4 5]\n",
      "[2 3 4 5] [0 1 6 7]\n",
      "[4 5 6 7] [0 1 2 3]\n"
     ]
    }
   ],
   "source": [
    ">>> from sklearn.model_selection import GroupShuffleSplit\n",
    ">>> X = [0.1, 0.2, 2.2, 2.4, 2.3, 4.55, 5.8, 0.001]\n",
    ">>> y = [\"a\", \"b\", \"b\", \"b\", \"c\", \"c\", \"c\", \"a\"]\n",
    ">>> groups = [1, 1, 2, 2, 3, 3, 4, 4]\n",
    ">>> gss = GroupShuffleSplit(n_splits=4, test_size=0.5, random_state=0)\n",
    ">>> for train, test in gss.split(X, y, groups=groups): print(\"%s %s\" % (train, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TimeSeriesSplit  \n",
    "TimeSeriesSplit is a variation of k-fold which returns first k folds as train set and the (k+1) th fold as test set. Note that unlike standard cross-validation methods, successive training sets are supersets of those that come before them. Also, it adds all surplus data to the first training partition, which is always used to train the model.\n",
    "\n",
    "This class can be used to cross-validate time series data samples that are observed at fixed time intervals.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2] [3]\n",
      "[0 1 2 3] [4]\n",
      "[0 1 2 3 4] [5]\n"
     ]
    }
   ],
   "source": [
    ">>> from sklearn.model_selection import TimeSeriesSplit\n",
    ">>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\n",
    ">>> y = np.array([1, 2, 3, 4, 5, 6])\n",
    ">>> tscv = TimeSeriesSplit(n_splits=3)\n",
    ">>> for train, test in tscv.split(X): print(\"%s %s\" % (train, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tunning The HyperParameters Of An Estimator  \n",
    "Hyper-parameters are parameters that are not directly learnt within estimators. In scikit-learn they are passed as arguments to the constructor of the estimator classes. Typical examples include C, kernel and gamma for Support Vector Classifier, alpha for Lasso, etc.  \n",
    "\n",
    "It is possible and recommended to search the hyper-parameter space for the best cross validation score.\n",
    "\n",
    "Any parameter provided when constructing an estimator may be optimized in this manner. Specifically, to find the names and current values for all parameters for a given estimator, use: `estimator.get_params()`  \n",
    "\n",
    "A search consists of:\n",
    "\n",
    "1. an estimator (regressor or classifier such as sklearn.svm.SVC());\n",
    "2. a parameter space;\n",
    "3. a method for searching or sampling candidates;\n",
    "4. a cross-validation scheme; and\n",
    "5. a score function.  \n",
    "\n",
    "Note that it is common that a small subset of those parameters can have a large impact on the predictive or computation performance of the model while others can be left to their default values. It is recommended to read the docstring of the estimator class to get a finer understanding of their expected behavior, possibly by reading the enclosed reference to the literature.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exhaustive Grid Search  \n",
    "The grid search provided by GridSearchCV exhaustively generates candidates from a grid of parameter values specified with the param_grid parameter. For instance, the following param_grid:\n",
    "\n",
    "> ```\n",
    "> param_grid = [\n",
    ">   {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
    ">   {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},\n",
    "> ]  \n",
    "> ```\n",
    "\n",
    "The GridSearchCV instance implements the usual estimator API: when “fitting” it on a dataset all the possible combinations of parameter values are evaluated and the best combination is retained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Randomized Parameter Optimization  \n",
    "While using a grid of parameter settings is currently the most widely used method for parameter optimization, other search methods have more favourable properties. RandomizedSearchCV implements a randomized search over parameters, where each setting is sampled from a distribution over possible parameter values. This has two main benefits over an exhaustive search:\n",
    "\n",
    "A budget can be chosen independent of the number of parameters and possible values.\n",
    "Adding parameters that do not influence the performance does not decrease efficiency.\n",
    "Specifying how parameters should be sampled is done using a dictionary, very similar to specifying parameters for GridSearchCV. Additionally, a computation budget, being the number of sampled candidates or sampling iterations, is specified using the n_iter parameter. For each parameter, either a distribution over possible values or a list of discrete choices (which will be sampled uniformly) can be specified:  \n",
    "> ```\n",
    "> {'C': scipy.stats.expon(scale=100), 'gamma': scipy.stats.expon(scale=.1),\n",
    "  'kernel': ['rbf'], 'class_weight':['balanced', None]}\n",
    "  ```  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tips For Parameters Search  \n",
    "[Detail](http://scikit-learn.org/stable/modules/grid_search.html#tips-for-parameter-search)  \n",
    "1. Specifying an objective metric  \n",
    "2. Specifying multiple metrics for evaluation  \n",
    "3. Composite estimators and parameter spaces  \n",
    "4. Model selection: development and evaluation  \n",
    "5. Parallelism  \n",
    "6. Robustness to failure  \n",
    "\n",
    "#### Other  \n",
    "[Detail](http://scikit-learn.org/stable/modules/grid_search.html#alternatives-to-brute-force-parameter-search)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation: Quantifying the Quality of Predictions  \n",
    "3 different APIs for evaluating the quality of a model’s predictions:\n",
    "\n",
    "1. Estimator score method: Estimators have a score method providing a default evaluation criterion for the problem they are designed to solve\n",
    "2. Scoring parameter: Model-evaluation tools using cross-validation (such as model_selection.cross_val_score and model_selection.GridSearchCV) rely on an internal scoring strategy\n",
    "3. Metric functions: The metrics module implements functions assessing prediction error for specific purposes. These metrics are detailed in sections on Classification metrics, Multilabel ranking metrics, Regression metrics and Clustering metrics.  \n",
    "\n",
    "Get a baseline value of those metrics for random predictions:  \n",
    "\n",
    "*  Dummy estimators   \n",
    "#### `scoring` of CV  \n",
    "1. use [pre-defined](http://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-values)  \n",
    "2. customize scroing stragety from metric functions(make_scorer)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> from sklearn.metrics import fbeta_score, make_scorer\n",
    ">>> ftwo_scorer = make_scorer(fbeta_score, beta=2)\n",
    ">>> from sklearn.model_selection import GridSearchCV\n",
    ">>> from sklearn.svm import LinearSVC\n",
    ">>> grid = GridSearchCV(LinearSVC(), param_grid={'C': [1, 10]}, scoring=ftwo_scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6931471805599453"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> import numpy as np\n",
    ">>> def my_custom_loss_func(ground_truth, predictions):\n",
    "...     diff = np.abs(ground_truth - predictions).max()\n",
    "...     return np.log(1 + diff)\n",
    ">>> # loss_func will negate the return value of my_custom_loss_func,\n",
    ">>> #  which will be np.log(2), 0.693, given the values for ground_truth\n",
    ">>> #  and predictions defined below.\n",
    ">>> loss  = make_scorer(my_custom_loss_func, greater_is_better=False)\n",
    ">>> score = make_scorer(my_custom_loss_func, greater_is_better=True)\n",
    ">>> ground_truth = [[1], [1]]\n",
    ">>> predictions  = [0, 1]\n",
    ">>> from sklearn.dummy import DummyClassifier\n",
    ">>> clf = DummyClassifier(strategy='most_frequent', random_state=0)\n",
    ">>> clf = clf.fit(ground_truth, predictions)\n",
    ">>> loss(clf,ground_truth, predictions) \n",
    ">>> score(clf,ground_truth, predictions) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple Metric Evaluation\n",
    "specify multiple scoring metrics for the scoring parameter:  \n",
    "\n",
    "1. As an iterable of string metrics  \n",
    "2. As a dict mapping the scorer name to the scoring function  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12 13 15]\n",
      "[5 4 1]\n"
     ]
    }
   ],
   "source": [
    ">>> from sklearn.model_selection import cross_validate\n",
    ">>> from sklearn.metrics import confusion_matrix\n",
    ">>> # A sample toy binary classification dataset\n",
    ">>> X, y = datasets.make_classification(n_classes=2, random_state=0)\n",
    ">>> svm = LinearSVC(random_state=0)\n",
    ">>> def tp(y_true, y_pred): return confusion_matrix(y_true, y_pred)[0, 0]\n",
    ">>> def tn(y_true, y_pred): return confusion_matrix(y_true, y_pred)[0, 0]\n",
    ">>> def fp(y_true, y_pred): return confusion_matrix(y_true, y_pred)[1, 0]\n",
    ">>> def fn(y_true, y_pred): return confusion_matrix(y_true, y_pred)[0, 1]\n",
    ">>> scoring = {'tp' : make_scorer(tp), 'tn' : make_scorer(tn), 'fp' : make_scorer(fp), 'fn' : make_scorer(fn)}\n",
    ">>> cv_results = cross_validate(svm.fit(X, y), X, y, scoring=scoring)\n",
    ">>> # Getting the test set true positive scores\n",
    ">>> print(cv_results['test_tp'])          \n",
    ">>> # Getting the test set false negative scores\n",
    ">>> print(cv_results['test_fn'])          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Score:  Binary Classification   \n",
    "\n",
    "`precision_recall_curve(y_true, probas_pred)` - Compute precision-recall pairs for different probability thresholds\n",
    "`roc_curve(y_true, y_score[, pos_label, …])` - Compute Receiver operating characteristic (ROC)  \n",
    "`roc_auc_score` - Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores.  \n",
    "`average_precision_score` - Compute average precision (AP) from prediction scores  \n",
    "\n",
    "#### Classification Score: MultiClass  \n",
    "\n",
    "`cohen_kappa_score(y1, y2[, labels, weights, …])`- Cohen’s kappa: a statistic that measures inter-annotator agreement.\n",
    "`confusion_matrix(y_true, y_pred[, labels, …])` - Compute confusion matrix to evaluate the accuracy of a classification\n",
    "`hinge_loss(y_true, pred_decision[, labels, …])` - Average hinge loss (non-regularized)\n",
    "`matthews_corrcoef(y_true, y_pred[, …])` - Compute the Matthews correlation coefficient (MCC)  \n",
    "\n",
    "#### Classification Score: MultiLabel  \n",
    "`accuracy_score(y_true, y_pred[, normalize, …])` - Accuracy classification score.\n",
    "`classification_report(y_true, y_pred[, …])` - Build a text report showing the main classification metrics\n",
    "`f1_score(y_true, y_pred[, labels, …])` - Compute the F1 score, also known as balanced F-score or F-measure\n",
    "`beta_score(y_true, y_pred, beta[, labels, …])` - Compute the F-beta score\n",
    "`hamming_loss(y_true, y_pred[, labels, …])` - Compute the average Hamming loss.\n",
    "`jaccard_similarity_score(y_true, y_pred[, …])` - Jaccard similarity coefficient score\n",
    "`log_loss(y_true, y_pred[, eps, normalize, …])` - Log loss, aka logistic loss or cross-entropy loss.\n",
    "`precision_recall_fscore_support(y_true, y_pred)` - Compute precision, recall, F-measure and support for each class\n",
    "`precision_score(y_true, y_pred[, labels, …])` - Compute the precision\n",
    "`recall_score(y_true, y_pred[, labels, …])` - Compute the recall\n",
    "`zero_one_loss(y_true, y_pred[, normalize, …])` - Zero-one classification loss.\n",
    "`average_precision_score(y_true, y_score[, …])` - Compute average precision (AP) from prediction scores\n",
    "`roc_auc_score(y_true, y_score[, average, …])` - Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy Score  \n",
    "The accuracy_score function computes the accuracy, either the fraction (default) or the count (normalize=False) of correct predictions.\n",
    "\n",
    "In multilabel classification, the function returns the subset accuracy. If the entire set of predicted labels for a sample strictly match with the true set of labels, then the subset accuracy is 1.0; otherwise it is 0.0.\n",
    "\n",
    "If \\hat{y}_i is the predicted value of the i-th sample and y_i is the corresponding true value, then the fraction of correct predictions over n_\\text{samples} is defined as\n",
    "\n",
    "\\texttt{accuracy}(y, \\hat{y}) = \\frac{1}{n_\\text{samples}} \\sum_{i=0}^{n_\\text{samples}-1} 1(\\hat{y}_i = y_i)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "2\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    ">>> import numpy as np\n",
    ">>> from sklearn.metrics import accuracy_score\n",
    ">>> y_pred = [0, 2, 1, 3]\n",
    ">>> y_true = [0, 1, 2, 3]\n",
    ">>> print(accuracy_score(y_true, y_pred))\n",
    ">>> print(accuracy_score(y_true, y_pred, normalize=False))\n",
    ">>> print(accuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cohen’s kappa  \n",
    "This measure is intended to compare labelings by different human annotators, not a classifier versus a ground truth.\n",
    "\n",
    "The kappa score (see docstring) is a number between -1 and 1. Scores above .8 are generally considered good agreement; zero or lower means no agreement (practically random labels).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4285714285714286\n"
     ]
    }
   ],
   "source": [
    ">>> from sklearn.metrics import cohen_kappa_score\n",
    ">>> y_true = [2, 0, 2, 2, 0, 1]\n",
    ">>> y_pred = [0, 0, 2, 2, 0, 2]\n",
    ">>> print(cohen_kappa_score(y_true, y_pred))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion matrix  \n",
    " evaluates classification accuracy by computing the confusion matrix.\n",
    "\n",
    "By definition, entry i, j in a confusion matrix is the number of observations actually in group i, but predicted to be in group j  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 0 0]\n",
      " [0 0 1]\n",
      " [1 0 2]]\n"
     ]
    }
   ],
   "source": [
    ">>> from sklearn.metrics import confusion_matrix\n",
    ">>> y_true = [2, 0, 2, 2, 0, 1]\n",
    ">>> y_pred = [0, 0, 2, 2, 0, 2]\n",
    ">>> print(confusion_matrix(y_true, y_pred))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification report  \n",
    "builds a text report showing the main classification metrics  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.67      1.00      0.80         2\n",
      "    class 1       0.00      0.00      0.00         1\n",
      "    class 2       1.00      0.50      0.67         2\n",
      "\n",
      "avg / total       0.67      0.60      0.59         5\n",
      "\n"
     ]
    }
   ],
   "source": [
    ">>> from sklearn.metrics import classification_report\n",
    ">>> y_true = [0, 1, 2, 2, 0]\n",
    ">>> y_pred = [0, 0, 2, 1, 0]\n",
    ">>> target_names = ['class 0', 'class 1', 'class 2']\n",
    ">>> print(classification_report(y_true, y_pred, target_names=target_names))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hamming loss  \n",
    "omputes the average Hamming loss or Hamming distance between two sets of samples.\n",
    "\n",
    "If \\hat{y}_j is the predicted value for the j-th label of a given sample, y_j is the corresponding true value, and n_\\text{labels} is the number of classes or labels, then the Hamming loss L_{Hamming} between two samples is defined as:\n",
    "\n",
    "L_{Hamming}(y, \\hat{y}) = \\frac{1}{n_\\text{labels}} \\sum_{j=0}^{n_\\text{labels} - 1} 1(\\hat{y}_j \\not= y_j)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n",
      "0.75\n"
     ]
    }
   ],
   "source": [
    ">>> from sklearn.metrics import hamming_loss\n",
    ">>> y_pred = [1, 2, 3, 4]\n",
    ">>> y_true = [2, 2, 3, 4]\n",
    ">>> print(hamming_loss(y_true, y_pred))\n",
    ">>> print(hamming_loss(np.array([[0, 1], [1, 1]]), np.zeros((2, 2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jaccard similarity coefficient score  \n",
    " computes the average (default) or sum of Jaccard similarity coefficients, also called the Jaccard index, between pairs of label sets.\n",
    "\n",
    "The Jaccard similarity coefficient of the i-th samples, with a ground truth label set y_i and predicted label set \\hat{y}_i, is defined as\n",
    "\n",
    "J(y_i, \\hat{y}_i) = \\frac{|y_i \\cap \\hat{y}_i|}{|y_i \\cup \\hat{y}_i|}.  \n",
    "\n",
    "In binary and multiclass classification, the Jaccard similarity coefficient score is equal to the classification accuracy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "2\n"
     ]
    }
   ],
   "source": [
    ">>> import numpy as np\n",
    ">>> from sklearn.metrics import jaccard_similarity_score\n",
    ">>> y_pred = [0, 2, 1, 3]\n",
    ">>> y_true = [0, 1, 2, 3]\n",
    ">>> print(jaccard_similarity_score(y_true, y_pred))\n",
    ">>> print(jaccard_similarity_score(y_true, y_pred, normalize=False))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision, recall and F-measures  \n",
    "`precision` - the ability of the classifier not to label as positive a sample that is negative\n",
    "`recall` - the ability of the classifier to find all the positive samples  \n",
    "`F-measures` - nterpreted as a weighted harmonic mean of the precision and recall  \n",
    "\n",
    "`average_precision_score(y_true, y_score[, …])` - Compute average precision (AP) from prediction scores\n",
    "`f1_score(y_true, y_pred[, labels, …])` - Compute the F1 score, also known as balanced F-score or F-measure\n",
    "`fbeta_score(y_true, y_pred, beta[, labels, …])` - Compute the F-beta score\n",
    "`precision_recall_curve(y_true, probas_pred)` - Compute precision-recall pairs for different probability thresholds\n",
    "`precision_recall_fscore_support(y_true, y_pred)` - Compute precision, recall, F-measure and support for each class\n",
    "`precision_score(y_true, y_pred[, labels, …])` - Compute the precision\n",
    "`recall_score(y_true, y_pred[, labels, …])` - Compute the recall  \n",
    "\n",
    "[More Detail](http://scikit-learn.org/stable/modules/model_evaluation.html#precision-recall-and-f-measures)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More and More ...  \n",
    "[Detail](http://scikit-learn.org/stable/modules/model_evaluation.html#hinge-loss)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Presistence  \n",
    "After training a scikit-learn model, it is desirable to have a way to persist the model for future use without having to retrain. The following section gives you an example of how to persist a model with pickle. We’ll also review a few security and maintainability issues when working with pickle serialization.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    ">>> from sklearn import svm\n",
    ">>> from sklearn import datasets\n",
    ">>> clf = svm.SVC()\n",
    ">>> iris = datasets.load_iris()\n",
    ">>> X, y = iris.data, iris.target\n",
    ">>> clf.fit(X, y)\n",
    ">>> import pickle\n",
    ">>> s = pickle.dumps(clf)\n",
    ">>> clf2 = pickle.loads(s)\n",
    ">>> print(clf2.predict(X[0:1]))\n",
    ">>> print(y[0])\n",
    ">>> from sklearn.externals import joblib\n",
    ">>> joblib.dump(clf, 'filename.pkl') \n",
    ">>> clf = joblib.load('filename.pkl') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Curves: Plotting Scores to Evaluate Models  \n",
    "[Detail](http://scikit-learn.org/stable/modules/learning_curve.html)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Curve  \n",
    "Sometimes helpful to plot the influence of a single hyperparameter on the training score and the validation score to find out whether the estimator is overfitting or underfitting for some hyperparameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.94141575 0.92944161 0.92267644]\n",
      " [0.94141563 0.92944153 0.92267633]\n",
      " [0.47253778 0.45601093 0.42887489]] \n",
      "\n",
      " [[0.90335825 0.92525985 0.94159336]\n",
      " [0.90338529 0.92523396 0.94159078]\n",
      " [0.44639995 0.39639757 0.4567671 ]]\n"
     ]
    }
   ],
   "source": [
    ">>> import numpy as np\n",
    ">>> from sklearn.model_selection import validation_curve\n",
    ">>> from sklearn.datasets import load_iris\n",
    ">>> from sklearn.linear_model import Ridge\n",
    ">>> np.random.seed(0)\n",
    ">>> iris = load_iris()\n",
    ">>> X, y = iris.data, iris.target\n",
    ">>> indices = np.arange(y.shape[0])\n",
    ">>> np.random.shuffle(indices)\n",
    ">>> X, y = X[indices], y[indices]\n",
    ">>> train_scores, valid_scores = validation_curve(Ridge(), X, y, \"alpha\", np.logspace(-7, 3, 3))\n",
    ">>> print(train_scores, '\\n\\n', valid_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Curve  \n",
    "A learning curve shows the validation and training score of an estimator for varying numbers of training samples. It is a tool to find out how much we benefit from adding more training data and whether the estimator suffers more from a variance error or a bias error. If both the validation score and the training score converge to a value that is too low with increasing size of the training set, we will not benefit much from more training data   \n",
    "\n",
    "We will probably have to use an estimator or a parametrization of the current estimator that can learn more complex concepts (i.e. has a lower bias). If the training score is much greater than the validation score for the maximum number of training samples, adding more training samples will most likely increase generalization  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 50  80 110] \n",
      "\n",
      "\n",
      " [[0.98       0.98       0.98       0.98       0.98      ]\n",
      " [0.9875     1.         0.9875     0.9875     0.9875    ]\n",
      " [0.98181818 1.         0.98181818 0.98181818 0.99090909]] \n",
      "\n",
      "\n",
      " [[1.         0.93333333 1.         1.         0.96666667]\n",
      " [1.         0.96666667 1.         1.         0.96666667]\n",
      " [1.         0.96666667 1.         1.         0.96666667]]\n"
     ]
    }
   ],
   "source": [
    ">>> from sklearn.model_selection import learning_curve\n",
    ">>> from sklearn.svm import SVC\n",
    ">>> train_sizes, train_scores, valid_scores = learning_curve(SVC(kernel='linear'), X, y, train_sizes=[50, 80, 110], cv=5)\n",
    ">>> print(train_sizes, '\\n\\n\\n', train_scores, '\\n\\n\\n', valid_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategies to scale computationally: bigger data  \n",
    "[Detail](http://scikit-learn.org/stable/modules/scaling_strategies.html)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational Performance  \n",
    "[Detail](http://scikit-learn.org/stable/modules/computational_performance.html)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
