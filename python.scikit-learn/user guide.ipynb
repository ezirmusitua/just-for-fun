{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator For Regression  \n",
    "`make_regression` - produces regression targets as an optionally-sparse random linear combination of random features, with noise.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function sklearn.datasets.samples_generator.make_regression(n_samples=100, n_features=100, n_informative=10, n_targets=1, bias=0.0, effective_rank=None, tail_strength=0.5, noise=0.0, shuffle=True, coef=False, random_state=None)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.make_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`make_sparse_uncorrelated` - produces a target as a linear combination of four features with fixed coefficients  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function sklearn.datasets.samples_generator.make_sparse_uncorrelated(n_samples=100, n_features=10, random_state=None)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.make_sparse_uncorrelated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`make_friedman1` - related by polynomial and sine transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function sklearn.datasets.samples_generator.make_friedman1(n_samples=100, n_features=10, noise=0.0, random_state=None)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.make_friedman1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`make_friedman2` -  includes feature multiplication and reciprocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function sklearn.datasets.samples_generator.make_friedman2(n_samples=100, noise=0.0, random_state=None)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.make_friedman2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`make_friedman3` - similar with an arctan transformation on the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function sklearn.datasets.samples_generator.make_friedman3(n_samples=100, noise=0.0, random_state=None)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.make_friedman3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator For Manifold Learning  \n",
    "`make_s_curve` - Generate an S curve dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function sklearn.datasets.samples_generator.make_s_curve(n_samples=100, noise=0.0, random_state=None)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.make_s_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`make_swiss_roll` - Generate a swiss roll dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function sklearn.datasets.samples_generator.make_swiss_roll(n_samples=100, noise=0.0, random_state=None)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.make_swiss_roll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator For Decomposition  \n",
    "`make_low_rank_matrix` - Generate a mostly low rank matrix with bell-shaped singular values  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function sklearn.datasets.samples_generator.make_low_rank_matrix(n_samples=100, n_features=100, effective_rank=10, tail_strength=0.5, random_state=None)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.make_low_rank_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`make_sparse_coded_signal` - Generate a signal sas a sparse combination of dictionary elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function sklearn.datasets.samples_generator.make_sparse_coded_signal(n_samples, n_components, n_features, n_nonzero_coefs, random_state=None)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.make_sparse_coded_signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`make_spd_matrix` - Generate a random symmetric, positive-definite matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function sklearn.datasets.samples_generator.make_spd_matrix(n_dim, random_state=None)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.make_spd_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`make_sparse_spd_matrix` - Generate a sparse symmetric definite positive matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function sklearn.datasets.samples_generator.make_sparse_spd_matrix(dim=1, alpha=0.95, norm_diag=False, smallest_coef=0.1, largest_coef=0.9, random_state=None)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.make_sparse_spd_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets in svmlight / libsvm format \n",
    "svmlight/libsvm format: \\<label\\>  \\<feature-id\\>:\\<feature-value\\> \\<feature-id\\>: \\<feature-value\\> per line  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function sklearn.datasets.svmlight_format.load_svmlight_file(f, n_features=None, dtype=<class 'numpy.float64'>, multilabel=False, zero_based='auto', query_id=False, offset=0, length=-1)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.load_svmlight_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading From Extrenal Datasets  \n",
    "1. pandas.io\n",
    "2. scipy.io  \n",
    "3. numpy/routine.io  \n",
    "4. skimage.io / Imageio  \n",
    "5. scipy.misc.imread  \n",
    "6. scipy.io.wavfile.read  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading datasets from the mldata.org repository  \n",
    "`fetch_mldata`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function sklearn.datasets.mldata.fetch_mldata(dataname, target_name='label', data_name='data', transpose_data=True, data_home=None)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.fetch_mldata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Transformations  \n",
    "clean, reduce, expand, generate feature representations  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline And FeatureUnion: Cobining Estimators  \n",
    "Pipeline can use to chain estimators into one, useful when there is ofen a fixed sequence of steps in processing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to use  \n",
    "`Pipeline(estimators)`, estimators is a list of (key, value) tuple, key is the name of estimator, value is the estimator object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('reduce_im', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)), ('clf', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA  \n",
    "estimators = [('reduce_im', PCA()), ('clf', SVC())]  \n",
    "pipe = Pipeline(estimators)\n",
    "pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`make_pipeline(estimator1, estimator, ...)` shorthand for creating pipeline, name was autofilled  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('pca-1', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)), ('pca-2', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)), ('svc', SVC(C=1.0, cache_size...,\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline  \n",
    "pipe = make_pipeline(PCA(), PCA(), SVC())\n",
    "pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**steps/named_steps** attribute store the estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pca-1',\n",
       "  PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
       "    svd_solver='auto', tol=0.0, whiten=False)),\n",
       " ('pca-2',\n",
       "  PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
       "    svd_solver='auto', tol=0.0, whiten=False)),\n",
       " ('svc', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False))]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pca-1': PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
       "   svd_solver='auto', tol=0.0, whiten=False),\n",
       " 'pca-2': PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
       "   svd_solver='auto', tol=0.0, whiten=False),\n",
       " 'svc': SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "   decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "   max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "   tol=0.001, verbose=False)}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.named_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use **estimator__parameter** to access estimator's parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('pca-1', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)), ('pca-2', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)), ('svc', SVC(C=0, cache_size=2...,\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.set_params(svc__C=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following shows how to use GridSearchCV  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'pca-1__n_components': [2, 5, 10], 'svc__C': [0.1, 10, 100]}\n",
    "grid_search = GridSearchCV(pipe, param_grid=param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perfermance  \n",
    "pipeline will cache each transform after calling fit, so if parameters and input data are identical the tranformation wont running  \n",
    "`Pipeline(..., memory=dirname_or_jobmemoryobject)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempfile import mkdtemp\n",
    "from shutil import rmtree\n",
    "estimators = [('reduce_dim', PCA()), ('clf', SVC())]\n",
    "cachedir = mkdtemp()\n",
    "pipe = Pipeline(estimators, memory=cachedir)\n",
    "rmtree(cachedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FeatureUnion: Composite Feature Spaces  \n",
    "FeatureUnion combine several transformer objects into a new transformer that combines their output  \n",
    "while fitting, each estimator fit data indenpendently, the output are concatenated end-to-end into larger vectors  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('linear_pca', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)), ('kernel_pca', KernelPCA(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto',\n",
       "     fit_inverse_transform=False, gamma=None, kernel='linear',\n",
       "     kernel_params=None, max_iter=None, n_components=None, n_jobs=1,\n",
       "     random_state=None, remove_zero_eig=False, tol=0))],\n",
       "       transformer_weights=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.decomposition import KernelPCA\n",
    "estimators = [('linear_pca', PCA()), ('kernel_pca', KernelPCA())]\n",
    "combined = FeatureUnion(estimators)\n",
    "combined "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction  \n",
    "The sklearn.feature_extraction module can be used to extract features in a format supported by machine learning algorithms from datasets consisting of formats such as text and image.\n",
    "**Difference with Feature Selection**: the former consists in transforming arbitrary data, such as text or images, into numerical features usable for machine learning. The latter is a machine learning technique applied on these features  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Features From Dicts  \n",
    "`DictVectorizer` - convert feature arrays represented as liss of standard python dict object to numpy/scipy representation, it implement the `one-hot` coding for categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0. 33.]\n",
      " [ 0.  1.  0. 12.]\n",
      " [ 0.  0.  1. 18.]]\n",
      "['city=Dubai', 'city=London', 'city=San Francisco', 'temperature']\n"
     ]
    }
   ],
   "source": [
    ">>> measurements = [\n",
    "...     {'city': 'Dubai', 'temperature': 33.},\n",
    "...     {'city': 'London', 'temperature': 12.},\n",
    "...     {'city': 'San Francisco', 'temperature': 18.},\n",
    "... ]\n",
    "\n",
    ">>> from sklearn.feature_extraction import DictVectorizer\n",
    ">>> vec = DictVectorizer()\n",
    "\n",
    ">>> print(vec.fit_transform(measurements).toarray())\n",
    "\n",
    ">>> print(vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also is a useful representation transformation for training sequence classifiers in NLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1.0\n",
      "  (0, 1)\t1.0\n",
      "  (0, 2)\t1.0\n",
      "  (0, 3)\t1.0\n",
      "  (0, 4)\t1.0\n",
      "  (0, 5)\t1.0\n",
      "[[1. 1. 1. 1. 1. 1.]]\n",
      "['pos+1=PP', 'pos-1=NN', 'pos-2=DT', 'word+1=on', 'word-1=cat', 'word-2=the']\n"
     ]
    }
   ],
   "source": [
    ">>> pos_window = [\n",
    "...     {\n",
    "...         'word-2': 'the',\n",
    "...         'pos-2': 'DT',\n",
    "...         'word-1': 'cat',\n",
    "...         'pos-1': 'NN',\n",
    "...         'word+1': 'on',\n",
    "...         'pos+1': 'PP',\n",
    "...     },\n",
    "...     # in a real application one would extract many such dictionaries\n",
    "... ]\n",
    ">>> vec = DictVectorizer()\n",
    ">>> pos_vectorized = vec.fit_transform(pos_window)\n",
    ">>> print(pos_vectorized)\n",
    ">>> print(pos_vectorized.toarray())\n",
    ">>> print(vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Hashing  \n",
    "An implementation of Feature hashing, apply a hash function to features to determine their column index in sample matrices directly  \n",
    "About collisions: use a signed hash function   \n",
    "Accept mappings or (feature, value) pair or strings  \n",
    "Output scipy.sparse  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 729803)\t-1.0\n",
      "  (0, 740061)\t1.0\n",
      "  (0, 892359)\t-1.0\n",
      "  (0, 950346)\t-1.0\n",
      "  (0, 1002789)\t-1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import FeatureHasher\n",
    "def token_features(token, part_of_speech):\n",
    "    if token.isdigit():\n",
    "        yield \"numeric\"\n",
    "    else:\n",
    "        yield \"token={}\".format(token.lower())\n",
    "        yield \"token,pos={},{}\".format(token, part_of_speech)\n",
    "    if token[0].isupper():\n",
    "        yield \"uppercase_initial\"\n",
    "    if token.isupper():\n",
    "        yield \"all_uppercase\"\n",
    "    yield \"pos={}\".format(part_of_speech)\n",
    "raw_X = (token_features(tok, pos) for tok, pos in [('A', 5)])\n",
    "hasher = FeatureHasher(input_type='string')\n",
    "X = hasher.transform(raw_X)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Feature Extraction  \n",
    "Extract numerical features from text content - **Vercorization**   \n",
    "Bag of words/Bag of n-grams Representation  \n",
    "1. tokenizing  \n",
    "2. counting  \n",
    "3. normalizing  \n",
    "\n",
    "#### Sparsity  \n",
    "Words in documents is a very small subset, use sparse to store it in order to save memory and fasten \n",
    "\n",
    "#### CountVectorizer  \n",
    "Implement both tokenization and occurrence counting  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 9)\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    ">>> corpus = [\n",
    "...     'This is the first document.',\n",
    "...     'This is the second second document.',\n",
    "...     'And the third one.',\n",
    "...     'Is this the first document?',\n",
    "... ]\n",
    ">>> X = vectorizer.fit_transform(corpus)\n",
    "print(X.shape)  \n",
    ">>> analyze = vectorizer.build_analyzer()\n",
    ">>> analyze(\"This is a text document to analyze.\") == (\n",
    "...     ['this', 'is', 'text', 'document', 'to', 'analyze'])\n",
    ">>> vectorizer.get_feature_names() == (\n",
    "...     ['and', 'document', 'first', 'is', 'one',\n",
    "...      'second', 'the', 'third', 'this'])\n",
    ">>> print(vectorizer.vocabulary_.get('document'))\n",
    ">>> vectorizer.transform(['Something completely new.']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract 2-grams of words in order to preserve some of the local ordering infomation  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0],\n",
       "       [0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> bigram_vectorizer = CountVectorizer(ngram_range=(1, 2),\n",
    "...                                     token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    ">>> analyze = bigram_vectorizer.build_analyzer()\n",
    ">>> analyze('Bi-grams are cool!') == (\n",
    "...     ['bi', 'grams', 'are', 'cool', 'bi grams', 'grams are', 'are cool'])\n",
    ">>> X_2 = bigram_vectorizer.fit_transform(corpus).toarray()\n",
    ">>> X_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF Term Weighting  \n",
    "Why? some words will be very present (e.g. “the”, “a”, “is” in English) hence carrying very little meaningful information about the actual contents of the document  \n",
    "What? \n",
    "> tf-idf(t, d) = tf(t, d) x idf(t)  \n",
    "> tf means term-frequency, idf means term-frequency times inverse document-frequency  \n",
    "> idf(t) = log((1+n<d\\>) / (1+df(d, t)) + 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.38629436 2.38629436 1.28768207 1.69314718 1.69314718 1.28768207\n",
      " 1.69314718 2.38629436 2.38629436 2.38629436 2.38629436 2.38629436\n",
      " 1.         1.69314718 2.38629436 2.38629436 2.38629436 2.38629436\n",
      " 1.28768207 1.69314718 2.38629436]\n",
      "[[0.         0.         0.28574186 0.37571621 0.37571621 0.28574186\n",
      "  0.37571621 0.         0.         0.         0.         0.\n",
      "  0.22190405 0.37571621 0.         0.         0.         0.\n",
      "  0.28574186 0.37571621 0.        ]\n",
      " [0.         0.         0.1793146  0.         0.         0.1793146\n",
      "  0.23577716 0.         0.         0.66460105 0.33230052 0.33230052\n",
      "  0.13925379 0.         0.33230052 0.         0.         0.\n",
      "  0.1793146  0.23577716 0.        ]\n",
      " [0.40240191 0.40240191 0.         0.         0.         0.\n",
      "  0.         0.         0.40240191 0.         0.         0.\n",
      "  0.16863046 0.         0.         0.40240191 0.40240191 0.40240191\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.25271307 0.33228732 0.33228732 0.25271307\n",
      "  0.         0.4683204  0.         0.         0.         0.\n",
      "  0.19625424 0.33228732 0.         0.         0.         0.\n",
      "  0.25271307 0.         0.4683204 ]]\n"
     ]
    }
   ],
   "source": [
    ">>> from sklearn.feature_extraction.text import TfidfTransformer\n",
    ">>> transformer = TfidfTransformer(smooth_idf=False)\n",
    ">>> tfidf = transformer.fit_transform(X_2)\n",
    ">>> print(transformer.idf_)\n",
    ">>> print(tfidf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoding Text Files  \n",
    "`CountVectorizer`take the encoding params(default is utf-8)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n"
     ]
    }
   ],
   "source": [
    ">>> import chardet    \n",
    ">>> text1 = b\"Sei mir gegr\\xc3\\xbc\\xc3\\x9ft mein Sauerkraut\"\n",
    ">>> text2 = b\"holdselig sind deine Ger\\xfcche\"\n",
    ">>> text3 = b\"\\xff\\xfeA\\x00u\\x00f\\x00 \\x00F\\x00l\\x00\\xfc\\x00g\\x00e\\x00l\\x00n\\x00 \\x00d\\x00e\\x00s\\x00 \\x00G\\x00e\\x00s\\x00a\\x00n\\x00g\\x00e\\x00s\\x00,\\x00 \\x00H\\x00e\\x00r\\x00z\\x00l\\x00i\\x00e\\x00b\\x00c\\x00h\\x00e\\x00n\\x00,\\x00 \\x00t\\x00r\\x00a\\x00g\\x00 \\x00i\\x00c\\x00h\\x00 \\x00d\\x00i\\x00c\\x00h\\x00 \\x00f\\x00o\\x00r\\x00t\\x00\"\n",
    ">>> decoded = [x.decode(chardet.detect(x)['encoding'])\n",
    "...            for x in (text1, text2, text3)]        \n",
    ">>> v = CountVectorizer().fit(decoded).vocabulary_    \n",
    ">>> for term in v: print(v)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More  \n",
    "1. [Applications And Examples](http://scikit-learn.org/stable/modules/feature_extraction.html#applications-and-examples)  \n",
    "2. [Limitations of the Bag of Words Representation](http://scikit-learn.org/stable/modules/feature_extraction.html#limitations-of-the-bag-of-words-representation)  \n",
    "3. [Vectorizing a large text corpus with hashing trick](http://scikit-learn.org/stable/modules/feature_extraction.html#vectorizing-a-large-text-corpus-with-the-hashing-trick)  \n",
    "4. [Performing out-of-core scaling with HashingVectorizer](http://scikit-learn.org/stable/modules/feature_extraction.html#performing-out-of-core-scaling-with-hashingvectorizer)  \n",
    "5. [Customizing the vectorizer classes](http://scikit-learn.org/stable/modules/feature_extraction.html#customizing-the-vectorizer-classes)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Feature Extract  \n",
    "#### Patch Extraction  \n",
    "`extract_patches_2d` - extract patches from image stored as a 2d array/3d(color) array  \n",
    "`reconstruct_from_patches_2d` - reconstruct image  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  3  6  9]\n",
      " [12 15 18 21]\n",
      " [24 27 30 33]\n",
      " [36 39 42 45]]\n",
      "(2, 2, 2, 3)\n",
      "[[[ 0  3]\n",
      "  [12 15]]\n",
      "\n",
      " [[15 18]\n",
      "  [27 30]]]\n",
      "(9, 2, 2, 3)\n",
      "[[15 18]\n",
      " [27 30]]\n"
     ]
    }
   ],
   "source": [
    ">>> import numpy as np\n",
    ">>> from sklearn.feature_extraction import image\n",
    ">>> one_image = np.arange(4 * 4 * 3).reshape((4, 4, 3))\n",
    ">>> print(one_image[:, :, 0])  # R channel of a fake RGB picture\n",
    ">>> patches = image.extract_patches_2d(one_image, (2, 2), max_patches=2, random_state=0)\n",
    ">>> print(patches.shape)\n",
    ">>> print(patches[:, :, :, 0])\n",
    ">>> patches = image.extract_patches_2d(one_image, (2, 2))\n",
    ">>> print(patches.shape)\n",
    ">>> print(patches[4, :, :, 0])\n",
    ">>> reconstructed = image.reconstruct_from_patches_2d(patches, (4, 4, 3))\n",
    ">>> np.testing.assert_array_equal(one_image, reconstructed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PatchExtractor` - like patch extractor, but support mult-image, is an estimator, can be use in pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 2, 2, 3)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> five_images = np.arange(5 * 4 * 4 * 3).reshape(5, 4, 4, 3)\n",
    ">>> patches = image.PatchExtractor((2, 2)).transform(five_images)\n",
    ">>> patches.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connectivity Graph of an Image  \n",
    "[Detail](http://scikit-learn.org/stable/modules/feature_extraction.html#connectivity-graph-of-an-image)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Data  \n",
    "standardize data\n",
    "#### Standardization or mean removal and variance scaling  \n",
    "In practice we often ignore the shape of the distribution and just **transform the data to center it by removing the mean value of each feature**, then **scale it by dividing non-constant features by their standard deviation**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scale` - a quick and easy way to perform standardization, scaled data has zero mean and unit variance \n",
    "`StandardScaler` - kind of estimator that implement scale  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         -1.22474487  1.33630621]\n",
      " [ 1.22474487  0.         -0.26726124]\n",
      " [-1.22474487  1.22474487 -1.06904497]]\n",
      "[1.         0.         0.33333333]\n",
      "[[-2.44948974  1.22474487 -0.26726124]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import scale as sk_scale, normalize as sk_normalize, StandardScaler,MinMaxScaler,MaxAbsScaler,QuantileTransformer,Normalizer,Binarizer, OneHotEncoder\n",
    "import numpy as np\n",
    "X_train = np.array([[ 1., -1.,  2.], [ 2.,  0.,  0.], [ 0.,  1., -1.]])\n",
    "X_scaled = sk_scale(X_train)\n",
    "print(X_scaled)  \n",
    "scaler = StandardScaler().fit(X_train)\n",
    "print(scaler.mean_)\n",
    "X_test = [[-1., 1., 0.]]\n",
    "print(scaler.transform(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MinMaxScaler/MaxAbsScaler` - scale features to lie between a given minimum and maximum value  \n",
    "For `MinMaxScaler`  \n",
    "> X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
    "> X_scaled = X_std * (max - min) + min  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 0.        , 1.        ],\n",
       "       [1.        , 0.5       , 0.33333333],\n",
       "       [0.        , 1.        , 0.        ]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> X_train = np.array([[ 1., -1.,  2.],[ 2.,  0.,  0.],[ 0.,  1., -1.]])\n",
    ">>> min_max_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    ">>> X_train_minmax = min_max_scaler.fit_transform(X_train)\n",
    ">>> X_train_minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5 -1.   1. ]\n",
      " [ 1.   0.   0. ]\n",
      " [ 0.   1.  -0.5]]\n",
      "[[-1.5 -1.   2. ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2., 1., 2.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> X_train = np.array([[ 1., -1.,  2.],[ 2.,  0.,  0.],[ 0.,  1., -1.]])\n",
    ">>> max_abs_scaler = MaxAbsScaler()\n",
    ">>> X_train_maxabs = max_abs_scaler.fit_transform(X_train)\n",
    ">>> print(X_train_maxabs)\n",
    ">>> X_test = np.array([[ -3., -1.,  4.]])\n",
    ">>> X_test_maxabs = max_abs_scaler.transform(X_test)\n",
    ">>> print(X_test_maxabs)\n",
    ">>> max_abs_scaler.scale_         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`robust_scale/RobustScaler`- data contains many outliers, scaling using the mean and variance of the data is likely to not work very well, They use more robust estimates for the center and range of your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-Linear Transformation  \n",
    "`QuantileTransformer/quantile_transform` provide a non-parametric transformation based on the quantile function to map the data to a uniform distribution with values between 0 and 1  \n",
    "It is also possible to map the transformed data to a normal distribution by setting output_distribution='normal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.3 5.1 5.8 6.5 7.9]\n",
      "[[4.3        2.         1.         0.1       ]\n",
      " [4.31491491 2.02982983 1.01491491 0.1       ]\n",
      " [4.32982983 2.05965966 1.02982983 0.1       ]\n",
      " ...\n",
      " [7.84034034 4.34034034 6.84034034 2.5       ]\n",
      " [7.87017017 4.37017017 6.87017017 2.5       ]\n",
      " [7.9        4.4        6.9        2.5       ]]\n"
     ]
    }
   ],
   "source": [
    ">>> from sklearn.datasets import load_iris\n",
    ">>> from sklearn.model_selection import train_test_split\n",
    ">>> iris = load_iris()\n",
    ">>> X, y = iris.data, iris.target\n",
    ">>> X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    ">>> quantile_transformer = QuantileTransformer(random_state=0)\n",
    ">>> X_train_trans = quantile_transformer.fit_transform(X_train)\n",
    ">>> X_test_trans = quantile_transformer.transform(X_test)\n",
    ">>> print(np.percentile(X_train[:, 0], [0, 25, 50, 75, 100]))\n",
    ">>> quantile_transformer = QuantileTransformer(\n",
    "...     output_distribution='normal', random_state=0)\n",
    ">>> X_trans = quantile_transformer.fit_transform(X)\n",
    ">>> print(quantile_transformer.quantiles_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization  \n",
    "Normalization is the process of scaling individual samples to have unit norm  \n",
    "This process can be useful if you plan to use a quadratic form such as the dot-product or any other kernel to quantify the similarity of any pair of samples  \n",
    "`normalize/Normalizer(scioy.sparse-liked-input)` - provides a quick and easy way to perform this operation on a single array-like dataset, either using the l1 or l2 norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.40824829 -0.40824829  0.81649658]\n",
      " [ 1.          0.          0.        ]\n",
      " [ 0.          0.70710678 -0.70710678]]\n",
      "[[ 0.40824829 -0.40824829  0.81649658]\n",
      " [ 1.          0.          0.        ]\n",
      " [ 0.          0.70710678 -0.70710678]]\n",
      "[[-0.70710678  0.70710678  0.        ]]\n"
     ]
    }
   ],
   "source": [
    ">>> X = [[ 1., -1.,  2.],[ 2.,  0.,  0.],[ 0.,  1., -1.]]\n",
    ">>> X_normalized = sk_normalize(X, norm='l2')\n",
    ">>> print(X_normalized)\n",
    ">>> normalizer = Normalizer().fit(X)  # fit does nothing\n",
    ">>> print(normalizer.transform(X))\n",
    ">>> print(normalizer.transform([[-1.,  1., 0.]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binarization  \n",
    "Feature binarization is the process of thresholding numerical features to get boolean values  \n",
    "This can be useful for downstream probabilistic estimators that make assumption that the input data is distributed according to a multi-variate Bernoulli distribution  \n",
    "It is also common among the text processing community to use binary feature values (probably to simplify the probabilistic reasoning) even if normalized counts (a.k.a. term frequencies) or TF-IDF valued features often perform slightly better in practice.  \n",
    "`Binarizer(threshold)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "[[0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    ">>> X = [[ 1., -1.,  2.],\n",
    "...      [ 2.,  0.,  0.],\n",
    "...      [ 0.,  1., -1.]]\n",
    "\n",
    ">>> binarizer = Binarizer().fit(X)  # fit does nothing\n",
    ">>> print(binarizer.transform(X))\n",
    ">>> binarizer = Binarizer(threshold=1.1)\n",
    ">>> print(binarizer.transform(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding Categorical Features  \n",
    "Convert categorical features to features that can be used with scikit-learn estimators\n",
    " `OneHotEncoder` - implemented the `one-of-K/one-hot` encoding,  transforms each categorical feature with m possible values into m binary features, with only one active( if there is a possibility that the training data might have missing categorical features, one has to explicitly set n_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 1. 0. 0. 0. 0. 1.]]\n",
      "OneHotEncoder(categorical_features='all', dtype=<class 'numpy.float64'>,\n",
      "       handle_unknown='error', n_values=[2, 3, 4], sparse=True)\n"
     ]
    }
   ],
   "source": [
    ">>> enc = OneHotEncoder()\n",
    ">>> enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])\n",
    ">>> print(enc.transform([[0, 1, 3]]).toarray())\n",
    ">>> enc = OneHotEncoder(n_values=[2, 3, 4])\n",
    ">>> # Note that there are missing categorical values for the 2nd and 3rd\n",
    ">>> # features\n",
    ">>> print(enc.fit([[1, 2, 3], [0, 2, 0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation(填充)  \n",
    "How to handle missing value in dataset?  \n",
    "\n",
    "1. Discard entire rows and/or columns containing missing values  \n",
    "2. Impute the missing values(i.e., to infer them from the known part of the data)  \n",
    "\n",
    "`Imputer` - provides basic strategies for imputing missing values, either using the mean, the median or the most frequent value of the row or column in which the missing values are located, this class also allows for different missing values encodings, support sparse matrices    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.         2.        ]\n",
      " [6.         3.66666667]\n",
      " [7.         6.        ]]\n",
      "[[4.         2.        ]\n",
      " [6.         3.66666667]\n",
      " [7.         6.        ]]\n"
     ]
    }
   ],
   "source": [
    ">>> import numpy as np\n",
    ">>> from sklearn.preprocessing import Imputer\n",
    ">>> imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    ">>> imp.fit([[1, 2], [np.nan, 3], [7, 6]])\n",
    ">>> X = [[np.nan, 2], [6, np.nan], [7, 6]]\n",
    ">>> print(imp.transform(X))  \n",
    ">>> import scipy.sparse as sp\n",
    ">>> X = sp.csc_matrix([[1, 2], [0, 3], [7, 6]])\n",
    ">>> imp = Imputer(missing_values=0, strategy='mean', axis=0)\n",
    ">>> imp.fit(X)\n",
    ">>> X_test = sp.csc_matrix([[0, 2], [6, 0], [7, 6]])\n",
    ">>> print(imp.transform(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating Polynomial Features  \n",
    "Often it’s useful to add complexity to the model by considering nonlinear features of the input data. A simple and common method to use is polynomial features, which can get features’ high-order and interaction terms  \n",
    "`PolynomialFeatures`  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1]\n",
      " [2 3]\n",
      " [4 5]]\n",
      "[[ 1.  0.  1.  0.  0.  1.]\n",
      " [ 1.  2.  3.  4.  6.  9.]\n",
      " [ 1.  4.  5. 16. 20. 25.]]\n",
      "[[0 1 2]\n",
      " [3 4 5]\n",
      " [6 7 8]]\n",
      "[[  1.   0.   1.   2.   0.   0.   2.   0.]\n",
      " [  1.   3.   4.   5.  12.  15.  20.  60.]\n",
      " [  1.   6.   7.   8.  42.  48.  56. 336.]]\n"
     ]
    }
   ],
   "source": [
    ">>> import numpy as np\n",
    ">>> from sklearn.preprocessing import PolynomialFeatures\n",
    ">>> X = np.arange(6).reshape(3, 2)\n",
    ">>> print(X)\n",
    ">>> poly = PolynomialFeatures(2)\n",
    ">>> print(poly.fit_transform(X))\n",
    ">>> X = np.arange(9).reshape(3, 3)\n",
    ">>> print(X)\n",
    ">>> poly = PolynomialFeatures(degree=3, interaction_only=True)\n",
    ">>> print(poly.fit_transform(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Transformer  \n",
    "`FunctionTransformer` - Convert an existing Python function into a transformer to assist in data cleaning or processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.69314718]\n",
      " [1.09861229 1.38629436]]\n"
     ]
    }
   ],
   "source": [
    ">>> import numpy as np\n",
    ">>> from sklearn.preprocessing import FunctionTransformer\n",
    ">>> transformer = FunctionTransformer(np.log1p)\n",
    ">>> X = np.array([[0, 1], [2, 3]])\n",
    ">>> print(transformer.transform(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UnSupervised Dimensionality Reduction  \n",
    "[Detail](http://scikit-learn.org/stable/modules/unsupervised_reduction.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Projection  \n",
    "Reduce the dimensionality of the data by trading a controlled amount of accuracy, for faster processing times and smaller model sizes.\n",
    "\n",
    "#### The Johnson-Lindenstrauss Lemma  \n",
    "The main theoretical result behind the efficiency of random projection is the Johnson-Lindenstrauss lemma (quoting Wikipedia):\n",
    "> In mathematics, the Johnson-Lindenstrauss lemma is a result concerning low-distortion embeddings of points from high-dimensional into low-dimensional Euclidean space. The lemma states that a small set of points in a high-dimensional space can be embedded into a space of much lower dimension in such a way that distances between the points are nearly preserved. The map used for the embedding is at least Lipschitz, and can even be taken to be an orthogonal projection.   \n",
    "\n",
    "`johnson_lindenstrauss_min_dim` - Knowing only the number of sample, estimates conservatively the minimal size of the random subspace to guarantee a bounded distortion introduced by the random projection  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "663\n",
      "[    663   11841 1112658]\n",
      "[ 7894  9868 11841]\n"
     ]
    }
   ],
   "source": [
    ">>> from sklearn.random_projection import johnson_lindenstrauss_min_dim\n",
    ">>> print(johnson_lindenstrauss_min_dim(n_samples=1e6, eps=0.5))\n",
    ">>> print(johnson_lindenstrauss_min_dim(n_samples=1e6, eps=[0.5, 0.1, 0.01]))\n",
    ">>> print(johnson_lindenstrauss_min_dim(n_samples=[1e4, 1e5, 1e6], eps=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian Random Projection  \n",
    "`GaussianRandomProjection` - reduces the dimensionality by projecting the original input space on a randomly generated matrix where components are drawn from the following distribution N(0, \\frac{1}{n_{components}})  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 3947)\n"
     ]
    }
   ],
   "source": [
    ">>> import numpy as np\n",
    ">>> from sklearn import random_projection\n",
    ">>> X = np.random.rand(100, 10000)\n",
    ">>> transformer = random_projection.GaussianRandomProjection()\n",
    ">>> X_new = transformer.fit_transform(X)\n",
    ">>> print(X_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sparse Random Projection  \n",
    "`SparseRandomProjection` - reduces the dimensionality by projecting the original input space using a sparse random matrix  \n",
    "[Detail](http://scikit-learn.org/stable/modules/random_projection.html#sparse-random-projection)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 3947)\n"
     ]
    }
   ],
   "source": [
    ">>> import numpy as np\n",
    ">>> from sklearn import random_projection\n",
    ">>> X = np.random.rand(100,10000)\n",
    ">>> transformer = random_projection.SparseRandomProjection()\n",
    ">>> X_new = transformer.fit_transform(X)\n",
    ">>> print(X_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Approximation  \n",
    "[Detail](http://scikit-learn.org/stable/modules/kernel_approximation.html)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise Metrics, Affinities, Kernels  \n",
    "evaluate pairwise distances or affinity of sets of samples(distance metrics and kernels)  \n",
    "all following functions under `sklearn.metrics.pairwise`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cosine Similarity  \n",
    "`cosine_similarity(sparse)`  \n",
    "cosine_similarity computes the L2-normalized dot product of vectors. That is, if x and y are row vectors, their cosine similarity k is defined as:\n",
    "\n",
    "k(x, y) = \\frac{x y^\\top}{\\|x\\| \\|y\\|}\n",
    "\n",
    "This is called cosine similarity, because Euclidean (L2) normalization projects the vectors onto the unit sphere, and their dot product is then the cosine of the angle between the points denoted by the vectors.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Kernel  \n",
    "`linear_kernel` - computes the linear kernel, that is, a special case of polynomial_kernel with degree=1 and coef0=0 (homogeneous). If x and y are column vectors, their linear kernel is:\n",
    "\n",
    "k(x, y) = x^\\top y  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polynomial kernel  \n",
    "`polynomial_kernel` - computes the degree-d polynomial kernel between two vectors. The polynomial kernel represents the similarity between two vectors. Conceptually, the polynomial kernels considers not only the similarity between vectors under the same dimension, but also across dimensions. When used in machine learning algorithms, this allows to account for feature interaction.\n",
    "\n",
    "The polynomial kernel is defined as:\n",
    "\n",
    "k(x, y) = (\\gamma x^\\top y +c_0)^d\n",
    "\n",
    "where:\n",
    "\n",
    "x, y are the input vectors\n",
    "d is the kernel degree\n",
    "If c_0 = 0 the kernel is said to be homogeneous.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sigmoid Kernel  \n",
    "`sigmoid_kernel` computes the sigmoid kernel between two vectors. The sigmoid kernel is also known as hyperbolic tangent, or Multilayer Perceptron (because, in the neural network field, it is often used as neuron activation function). It is defined as:\n",
    "\n",
    "k(x, y) = \\tanh( \\gamma x^\\top y + c_0)\n",
    "\n",
    "where:\n",
    "\n",
    "x, y are the input vectors\n",
    "\\gamma is known as slope\n",
    "c_0 is known as intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RBF Kernel  \n",
    "`rbf_kernel` computes the radial basis function (RBF) kernel between two vectors. This kernel is defined as:\n",
    "\n",
    "k(x, y) = \\exp( -\\gamma \\| x-y \\|^2)\n",
    "\n",
    "where x and y are the input vectors. If \\gamma = \\sigma^{-2} the kernel is known as the Gaussian kernel of variance \\sigma^2.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Laplacian Kernel  \n",
    "`laplacian_kernel` is a variant on the radial basis function kernel defined as:\n",
    "\n",
    "k(x, y) = \\exp( -\\gamma \\| x-y \\|_1)\n",
    "\n",
    "where x and y are the input vectors and \\|x-y\\|_1 is the Manhattan distance between the input vectors.\n",
    "\n",
    "It has proven useful in ML applied to noiseless data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chi-squared kernel  \n",
    "The chi-squared kernel is a very popular choice for training non-linear SVMs in computer vision applications. It can be computed using chi2_kernel and then passed to an sklearn.svm.SVC with kernel=\"precomputed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.36787944 0.89483932 0.58364548]\n",
      " [0.36787944 1.         0.51341712 0.83822343]\n",
      " [0.89483932 0.51341712 1.         0.7768366 ]\n",
      " [0.58364548 0.83822343 0.7768366  1.        ]]\n",
      "[0 1 0 1]\n",
      "or\n",
      "[0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    ">>> from sklearn.svm import SVC\n",
    ">>> from sklearn.metrics.pairwise import chi2_kernel\n",
    ">>> X = [[0, 1], [1, 0], [.2, .8], [.7, .3]]\n",
    ">>> y = [0, 1, 0, 1]\n",
    ">>> K = chi2_kernel(X, gamma=.5)\n",
    ">>> print(K)\n",
    ">>> svm = SVC(kernel='precomputed').fit(K, y)\n",
    ">>> print(svm.predict(K))\n",
    ">>> print('or')\n",
    ">>> svm = SVC(kernel=chi2_kernel).fit(X, y)\n",
    ">>> print(svm.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming The Prediction Target(y)  \n",
    "#### LabelBinarizer  \n",
    "create a label indicator matrix from a list of multi-class labels  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)\n",
      "[1 2 4 6]\n",
      "[[1 0 0 0]\n",
      " [0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    ">>> from sklearn import preprocessing\n",
    ">>> lb = preprocessing.LabelBinarizer()\n",
    ">>> print(lb.fit([1, 2, 6, 4, 2]))\n",
    ">>> print(lb.classes_)\n",
    ">>> print(lb.transform([1, 6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LabelEncoder  \n",
    "normalize labels such that they contain only values between 0 and n_classes-1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 6]\n",
      "[0 0 1 2]\n",
      "[1 1 2 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jferroal/.venvs/lite_data_science/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    ">>> from sklearn import preprocessing\n",
    ">>> le = preprocessing.LabelEncoder()\n",
    ">>> le.fit([1, 2, 2, 6])\n",
    ">>> print(le.classes_)\n",
    ">>> print(le.transform([1, 1, 2, 6]))\n",
    ">>> print(le.inverse_transform([0, 0, 1, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can also be used to transform non-numerical labels (as long as they are hashable and comparable) to numerical labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amsterdam', 'paris', 'tokyo']\n",
      "[2 2 1]\n",
      "['tokyo', 'tokyo', 'paris']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jferroal/.venvs/lite_data_science/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    ">>> le = preprocessing.LabelEncoder()\n",
    ">>> le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"])\n",
    ">>> print(list(le.classes_))\n",
    ">>> print(le.transform([\"tokyo\", \"tokyo\", \"paris\"]))\n",
    ">>> print(list(le.inverse_transform([2, 2, 1])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
